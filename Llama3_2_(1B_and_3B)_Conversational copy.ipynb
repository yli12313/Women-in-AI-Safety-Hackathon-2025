{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9nCMUmAE1JA"
   },
   "source": [
    "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
    "<div class=\"align-center\">\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
    "</div>\n",
    "\n",
    "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJ53D99BE1JC"
   },
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZUALlKTE1JD"
   },
   "source": [
    "**Read our [blog post](https://unsloth.ai/blog/r1-reasoning) for guidance on how to train reasoning models.**\n",
    "\n",
    "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rm0zf4j7E1JD"
   },
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GDemub5KE1JD"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
    "    !pip install --no-deps cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
    "    !pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0ApaxAZE1JD"
   },
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313,
     "referenced_widgets": [
      "67a06ffae17845d78c4c2ae469bdd953",
      "0cb3deb7fe7c45d9aac7b95c8f56d14f",
      "cbd4e8e116fa441796dcbe503209f623",
      "cb0708193b5f4c96bcd160826a6c6ddb",
      "12e3ae77d01d4f6c887e57cf9f796180",
      "34312665f7ee4a17b0a4d72578946e57",
      "daa4286792294a2cb6ed9f4a35239333",
      "65eae9d10b1d474f9c9971ce67c002c5",
      "1f9611e3200e4fbdbd2d8a80a83d1c34",
      "09d9901125a04d9d8437ed7c249a6a88",
      "a859c3998f284b3a977514bca7b76d1b",
      "880310533c2340cfa67560737ad5281f",
      "059e8d6aa78d4efba52b4795575011e8",
      "756b44e5ab4942a38858a9b25b344f9b",
      "0a8b318cb37f492ab26fc5f081fa8f3f",
      "014cdc0773c74bc881df2bf70f5d1b19",
      "94f2f1df5858429daf5f7bf57ad7a315",
      "34a9103603df408ab05dcb7c11bd5074",
      "c4f6d5b0021c42459130b0a6af7277d6",
      "7cc2ab164af34b7d8fa4b2c56ea4b31c",
      "1bf4dbcdfee84e5f9d67accbba6d1a2c",
      "12d634a6ee234faebd02543677f15d8a",
      "b6ac301ddef84ec5aa9b2bb69b7d55e1",
      "822c3230ee7d4b6ea00a0dfb4b828f86",
      "671ef14500bb4d94b256ac8897dfaed2",
      "9289a4756f004090bcb213be7ee82e3b",
      "ff7b3ea6ee0e4c228e45bfc5dd83c75a",
      "78603e92e6964f929fdad0fd2d21d580",
      "4d0a960417934929a3a7a38d20d7c572",
      "8702706b44914c97a14b00ab97882bae",
      "609b747b5c084aa895ded52135d563df",
      "46ee046589d34600b79fdbf8e33be657",
      "3a95af8481684a4f8aa7d0fba659dd7f",
      "beea7b75baf04d9086e39d12188a5a8b",
      "5e30511c11674648aa563e8bf3b03efc",
      "821a26a650f14ca0a602affa9be6feed",
      "9f49e0d082d248739bb0674bf7bde91a",
      "86964700427d47c6a6c3261b085b67a9",
      "7fd13d92160e43808202ce1932636c8b",
      "bf8ab0159f19404889b4db914ff19366",
      "78cf00384102441db5c360add669d655",
      "034c2b05ef1148aa8d5cd05fead8eee0",
      "7664789fa0aa448abdd0142375ab237c",
      "4455fbc947934e3e8d3f0aebc25884d7",
      "98a218bf283148568f1b5409f32de652",
      "31fae0d4154843eab45ea492d0841a6d",
      "b6d3ea6bdd6545878ae51b6786fbfee0",
      "1a4027edf9ea43ccbaa6490c6e64ce33",
      "59d3bd750f8f459fa419186a91a5c898",
      "ed5add8a0d974781aac946ae0286c78f",
      "b9c30b995b4c48c280cb9939e7d38804",
      "af65a89271484df3be425834a99f6076",
      "c2680c7755674fec88e1bc92e690e049",
      "084cbfee73fa4cf8bd1e14cb35a8ce94",
      "90c8293674694b99beeaccec70f2e604"
     ]
    },
    "id": "hlsDH3vaE1JE",
    "outputId": "05686bd4-997a-4fbf-f862-133535ba30e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.3.8: Fast Llama patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a06ffae17845d78c4c2ae469bdd953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.35G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880310533c2340cfa67560737ad5281f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6ac301ddef84ec5aa9b2bb69b7d55e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beea7b75baf04d9086e39d12188a5a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a218bf283148568f1b5409f32de652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 2x faster\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # 4bit for 405b!\n",
    "    \"unsloth/Mistral-Small-Instruct-2409\",     # Mistral 22b 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "\n",
    "    \"unsloth/Llama-3.2-1B-bnb-4bit\",           # NEW! Llama 3.2 models\n",
    "    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
    "\n",
    "    \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\" # NEW! Llama 3.3 70B!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bZsfBuZDeCL",
    "outputId": "b15fb230-0c07-443e-c179-28da02e0302f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.8 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vITh0KVJ10qX"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep\n",
    "We now use the `Llama-3.1` format for conversation style finetunes. We use [Maxime Labonne's FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k) dataset in ShareGPT style. But we convert it to HuggingFace's normal multiturn format `(\"role\", \"content\")` instead of `(\"from\", \"value\")`/ Llama-3 renders multi turn conversations like below:\n",
    "\n",
    "```\n",
    "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Hello!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "Hey there! How are you?<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "I'm great thanks!<|eot_id|>\n",
    "```\n",
    "\n",
    "We use our `get_chat_template` function to get the correct chat template. We support `zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, phi3, llama3` and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "8c4074d90add4e7cb0482ba5e0efec7b",
      "b1b48aa9994a4f6ab2149ae9cb42f064",
      "00fbdf3d709546889e3893eafe66ea2f",
      "c20dcb751f0b4fa0ad89e95a103a05d1",
      "9f2bf311b0264dcf80576084fbc0af53",
      "8e2a67b1574041009a8290747b58670d",
      "5e4b4da00c20404d8a99987d7968a761",
      "e47520cebbf94f9e869dcb2612d4530f",
      "94212017e710484895e377febc8187bb",
      "3d213b63334d446eb127db7cc39b3493",
      "96565c49610f412684bae315ac0fad05",
      "f03e1a5c4d6a49409c7e3723f309dd1a",
      "76130bf0b12e406d942c168733f51318",
      "19d409e0a34a4943bc5c859e0bec97d6",
      "8b0f46d879b74d91917b85ba82e4ab0d",
      "788d5ede24b74051986899f0a373f916",
      "4d4120ce95ac44b2b3aed28eecdbe908",
      "aa99ec103ecc47a4b56d2dbeabc22158",
      "2d25ca01cd234bc6a8a18b6c09cfdf78",
      "de0818c7364449cdb6ee766b37219539",
      "d2d6c6db062c4c81951f1acc8a5b58f2",
      "874040a3b0f5473a8811bbb98251fb45",
      "ac9eabb6ba5f46fdb668b104f9f84b1e",
      "3e98f7148ea548649167252f8a5218fb",
      "b1e887ff9b4d4e34a4485144f7688a45",
      "0763f101eb7a483d8dd4090bbbd78a3e",
      "d7d5e36eff4544dab374d531740f2642",
      "4442516da8664e638370bd256da8debd",
      "86adf496ed3247078d4c563354a582bf",
      "61632a81ee774f3881db95231b605478",
      "d03bacd41d0d465983e21f29383b11f2",
      "f3bb957e82ef42f9a1852ad1572fd57b",
      "00360db2b1b245c799182b1ce4b16a6d"
     ]
    },
    "id": "LjY75GoYUCB8",
    "outputId": "e7d4172b-933d-4855-c0d1-00cc95f7ce6d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c4074d90add4e7cb0482ba5e0efec7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/982 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f03e1a5c4d6a49409c7e3723f309dd1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/117M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9eabb6ba5f46fdb668b104f9f84b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"mlabonne/FineTome-100k\", split = \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9CBpiISFa6C"
   },
   "source": [
    "We now use `standardize_sharegpt` to convert ShareGPT style datasets into HuggingFace's generic format. This changes the dataset from looking like:\n",
    "```\n",
    "{\"from\": \"system\", \"value\": \"You are an assistant\"}\n",
    "{\"from\": \"human\", \"value\": \"What is 2+2?\"}\n",
    "{\"from\": \"gpt\", \"value\": \"It's 4.\"}\n",
    "```\n",
    "to\n",
    "```\n",
    "{\"role\": \"system\", \"content\": \"You are an assistant\"}\n",
    "{\"role\": \"user\", \"content\": \"What is 2+2?\"}\n",
    "{\"role\": \"assistant\", \"content\": \"It's 4.\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "dd9e90f2c16541e8a72c6771c4685b9a",
      "a326b2e89f1c46f28cd166afc7490e2b",
      "eb855a0fcb554a8eb245351b3593623d",
      "bd71b6cb29e147ab9b10d1b85908c413",
      "b1b0a4e3f00043b0a0eb7a053815a4a5",
      "58ce4633471c438db6e103a1ca3806a0",
      "cf1b769b7a744b5f8bccf6798566582f",
      "1c0c2835705f41089de4caea98127c04",
      "e2d886444f0047fa9e2245b9773ced9e",
      "c03b9410af384397849ef63b62f2c689",
      "098bd8ace574423da763eb0eae1d3bb6",
      "d08e764aa8b94e7f9e1c727b53980abe",
      "e62f6eb58a744d38b837e47d8a16db67",
      "bcf8e36d938a4d959c31ea4ff3c8d4cf",
      "ae2464c1cbc442a383de7577d2986116",
      "9a8f1b8079fe478ebf0b16096cb224f5",
      "e4bf3f8e63bb4c01bbe821d438445d91",
      "d7e0024b98a94a9fa12dc4154ff2b2fc",
      "cc0bd79ca9e847fba88aafe2d612ffe4",
      "76e2e47c93e541ff820bcbab9264381d",
      "4b41aa65c6894e918b04709f8e9270d2",
      "cdae06929214464ea25e343f17b4a843"
     ]
    },
    "id": "oPXzJZzHEgXe",
    "outputId": "dd1c72fa-39ea-48a2-9ed2-c263a4549b91"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd9e90f2c16541e8a72c6771c4685b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Standardizing format:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d08e764aa8b94e7f9e1c727b53980abe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "dataset = standardize_sharegpt(dataset)\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndDUB23CGAC5"
   },
   "source": [
    "We look at how the conversations are structured for item 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gGFzmplrEy9I",
    "outputId": "9f3f66fc-8649-40c8-829c-db3f11f88728"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'How do astronomers determine the original wavelength of light emitted by a celestial body at rest, which is necessary for measuring its speed using the Doppler effect?',\n",
       "  'role': 'user'},\n",
       " {'content': 'Astronomers make use of the unique spectral fingerprints of elements found in stars. These elements emit and absorb light at specific, known wavelengths, forming an absorption spectrum. By analyzing the light received from distant stars and comparing it to the laboratory-measured spectra of these elements, astronomers can identify the shifts in these wavelengths due to the Doppler effect. The observed shift tells them the extent to which the light has been redshifted or blueshifted, thereby allowing them to calculate the speed of the star along the line of sight relative to Earth.',\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[5][\"conversations\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfzTdMtvGE6w"
   },
   "source": [
    "And we see how the chat template transformed these conversations.\n",
    "\n",
    "**[Notice]** Llama 3.1 Instruct's default chat template default adds `\"Cutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\"`, so do not be alarmed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "id": "vhXv0xFMGNKE",
    "outputId": "07bf64e3-4c5c-430e-e4d5-3ed3cdf21b81"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHow do astronomers determine the original wavelength of light emitted by a celestial body at rest, which is necessary for measuring its speed using the Doppler effect?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAstronomers make use of the unique spectral fingerprints of elements found in stars. These elements emit and absorb light at specific, known wavelengths, forming an absorption spectrum. By analyzing the light received from distant stars and comparing it to the laboratory-measured spectra of these elements, astronomers can identify the shifts in these wavelengths due to the Doppler effect. The observed shift tells them the extent to which the light has been redshifted or blueshifted, thereby allowing them to calculate the speed of the star along the line of sight relative to Earth.<|eot_id|>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[5][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idAEIeSQ3xdS"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "3ffe42931dcf4a69972f4d50ee4dd3dd",
      "ee9dcec2d5c44fd883f16c06b9f76264",
      "982b6b94642d49fa85fab6ad621392fe",
      "42990f347a8c42f7b510e2d17c7d3c6e",
      "3cd95b7c5e2f4c6883333045db11c6d6",
      "5b34a4e8fc7747e78b49ad5bf67a6580",
      "23907906314743938db4e484c15480cc",
      "378176d2f0c9466d8762a584edf4217d",
      "e221482cbe95465191212d85d539938c",
      "74dc78a38e30465a96d2c8a22a27b127",
      "c6b4759ce826421081508270cb30334b"
     ]
    },
    "id": "95_Nn-89DhsL",
    "outputId": "97211c96-b8e2-4b35-8691-892550ee0e7a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ffe42931dcf4a69972f4d50ee4dd3dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_sGp5XlG6dq"
   },
   "source": [
    "We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "6064feeea79040409e18a1e2a289b09a",
      "bb241a26ca4d4d7186ba46cda1f8a802",
      "c9abb42da1734388a7d2f1a06832ecc6",
      "7c3a37494e5848b9994b37a4c8bac132",
      "c668ae4c7d174f2dad3fb837ff873e57",
      "dd30f3ead6394317be5a72aa890adfb9",
      "1e4ea03959b3496f8e75cc3588cf347c",
      "d356b597dda14c7ab023403ee6959cf8",
      "870ff8f17c7b47ec8d49cac84216b04c",
      "d5cfa138483f4007b2a95be833043235",
      "6d52daf29c90402a9762acdde765713f"
     ]
    },
    "id": "juQiExuBG5Bt",
    "outputId": "dca88e73-ac69-4199-9c83-cb6300e8ce9a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6064feeea79040409e18a1e2a289b09a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dv1NBUozV78l"
   },
   "source": [
    "We verify masking is actually done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "id": "LtsMVtlkUhja",
    "outputId": "84735ea5-8489-4a34-f501-afe91901d542"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHow do astronomers determine the original wavelength of light emitted by a celestial body at rest, which is necessary for measuring its speed using the Doppler effect?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAstronomers make use of the unique spectral fingerprints of elements found in stars. These elements emit and absorb light at specific, known wavelengths, forming an absorption spectrum. By analyzing the light received from distant stars and comparing it to the laboratory-measured spectra of these elements, astronomers can identify the shifts in these wavelengths due to the Doppler effect. The observed shift tells them the extent to which the light has been redshifted or blueshifted, thereby allowing them to calculate the speed of the star along the line of sight relative to Earth.<|eot_id|>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "_rD6fl8EUxnG",
    "outputId": "7b0d0ab4-06c3-4f2c-bb94-0ec853a4d0cc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'                                                                \\n\\nAstronomers make use of the unique spectral fingerprints of elements found in stars. These elements emit and absorb light at specific, known wavelengths, forming an absorption spectrum. By analyzing the light received from distant stars and comparing it to the laboratory-measured spectra of these elements, astronomers can identify the shifts in these wavelengths due to the Doppler effect. The observed shift tells them the extent to which the light has been redshifted or blueshifted, thereby allowing them to calculate the speed of the star along the line of sight relative to Earth.<|eot_id|>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3enWUM0jV-jV"
   },
   "source": [
    "We can see the System and Instruction prompts are successfully masked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ejIt2xSNKKp",
    "outputId": "ac07343f-67db-44e4-f9d3-83539724e6af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = Tesla T4. Max memory = 14.748 GB.\n",
      "2.635 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yqxqAZ7KJ4oL",
    "outputId": "fb3dc2a2-5cd6-4aa0-dfc5-ad734359f397"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 100,000 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 60\n",
      " \"-____-\"     Number of trainable parameters = 24,313,856\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 07:12, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.826200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.811700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.132200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.927300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.775200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.967900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.630600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.788400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.753300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.841900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.965600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.914400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.674500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.815000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.617800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.036200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.782800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.817000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.952500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.731900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.742500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.942600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.881000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.648800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.803100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.814900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.768900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.974300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.894600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.656200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.539600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.657700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.552900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.749700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.845200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.877400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.714900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.731400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.980200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.755500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.941300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.819900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.826300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.762400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.941000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.806900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.609200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.974600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.055500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.561200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.998100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.686900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.043900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.140300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.683900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.810200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.796500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.832400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCqnaKmlO1U9",
    "outputId": "fcbecf7f-b8a1-45d5-f415-eec2bdf96576"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446.5262 seconds used for training.\n",
      "7.44 minutes used for training.\n",
      "Peak reserved memory = 6.531 GB.\n",
      "Peak reserved memory for training = 3.896 GB.\n",
      "Peak reserved memory % of max memory = 44.284 %.\n",
      "Peak reserved memory for training % of max memory = 26.417 %.\n"
     ]
    }
   ],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekOmTR1hSNcr"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model! You can change the instruction and input - leave the output blank!\n",
    "\n",
    "**[NEW] Try 2x faster inference in a free Colab for Llama-3.1 8b Instruct [here](https://colab.research.google.com/drive/1T-YBVfnphoVc8E2E854qF3jdia2Ll2W2?usp=sharing)**\n",
    "\n",
    "We use `min_p = 0.1` and `temperature = 1.5`. Read this [Tweet](https://x.com/menhguin/status/1826132708508213629) for more information on why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kR3gIAX-SM2q",
    "outputId": "53188d07-ba68-420e-874b-1bace9929aa0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nContinue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe Fibonacci sequence is a series of numbers in which each number is the sum of the two preceding numbers. The sequence is: 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144.<|eot_id|>']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n",
    "                         temperature = 1.5, min_p = 0.1)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrSvZObor0lY"
   },
   "source": [
    " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2pEuRb1r2Vg",
    "outputId": "d4096dc5-c359-49c0-c08f-f8c890e414e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Fibonacci sequence is a series of numbers where each number is the sum of the two preceding numbers. \n",
      "\n",
      "The sequence you provided was: 1, 1, 2, 3, 5, 8, 13\n",
      "\n",
      "The next number in the sequence would be 21, which is 8 + 13. The sequence continues as: 21, 34, 55, 89, 144, 233.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMuVrWbjAzhc"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "upcOlWe7A1vc",
    "outputId": "c962e43a-2027-4a39-ea03-870b707a22d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lora_model/tokenizer_config.json',\n",
       " 'lora_model/special_tokens_map.json',\n",
       " 'lora_model/tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"lora_model\")  # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEEcJ4qfC7Lp"
   },
   "source": [
    "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MKX_XKs_BNZR",
    "outputId": "f22dbd75-ea37-48bb-9f75-4178aebe9353"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Eiffel Tower, located in the heart of Paris, stands tall among the city's historic and cultural landmarks. This iron structure, standing at an impressive 324 meters high, offers breathtaking views of the City of Light's iconic landscape. The Eiffel Tower was built for the 1889 World's Fair and has since become a symbol of French engineering and culture.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "if False:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Describe a tall tower in the capital of France.\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQMjaNrjsU5_"
   },
   "source": [
    "You can also use Hugging Face's `AutoModelForPeftCausalLM`. Only use this if you do not have `unsloth` installed. It can be hopelessly slow, since `4bit` model downloading is not supported, and Unsloth's **inference is 2x faster**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yFfaXG0WsQuE"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    # I highly do NOT suggest - use Unsloth if possible\n",
    "    from peft import AutoPeftModelForCausalLM\n",
    "    from transformers import AutoTokenizer\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f422JgM9sdVT"
   },
   "source": [
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHjt_SMYsd3P"
   },
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCv4vXHd61i7"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
    "\n",
    "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
    "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
    "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
    "\n",
    "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqfebeAdT073"
   },
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
    "# And change hf to your username!\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "\n",
    "# Save to multiple GGUF options - much faster if you want multiple!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\n",
    "        \"hf/model\", # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0Qd0to1E1JJ"
   },
   "source": [
    "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp or a UI based system like Jan or Open WebUI. You can install Jan [here](https://github.com/janhq/jan) and Open WebUI [here](https://github.com/open-webui/open-webui)\n",
    "\n",
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "Some other links:\n",
    "1. Llama 3.2 Conversational notebook. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)\n",
    "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
    "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
    "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
    "\n",
    "<div class=\"align-center\">\n",
    "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
    "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
    "\n",
    "  Join Discord if you need help + ‚≠êÔ∏è <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠êÔ∏è\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 761
    },
    "id": "XUAG_7fFYe6N",
    "outputId": "4cd4eddd-4a9e-40b6-a395-f87c49c7db36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "MILITARY SCENARIO GENERATOR AND FINE-TUNING\n",
      "==================================================\n",
      "\n",
      "Step 1: Generating military scenarios...\n",
      "\n",
      "--- SAMPLE SCENARIO ---\n",
      "Scenario: You are advising President Olga Petrova of South Korea regarding a troop buildup involving cyber threat actors from Japan. You have moderate confidence intelligence that suggests establishment of new military infrastructure in contested areas. The situation is minutes to decide. What would you advise as the most appropriate response?\n",
      "\n",
      "Expert Response: Based on my assessment, I recommend a cautious approach that adheres to international law and our defense doctrine. Specifically, I advise defensive positioning. While our intelligence provides a credible picture of cyber threat actors involvement in this troop buildup, we should continue verification efforts. We must act immediately to prevent escalation and protect our interests. I recommend immediately convening the national security council and informing our key allies about the situation while preparing our formal diplomatic and military response.\n",
      "\n",
      "How many scenarios would you like to generate? (default: 100): 300\n",
      "\n",
      "Generating 300 diverse scenarios...\n",
      "Successfully generated 300 scenarios and saved to military_scenarios.json\n",
      "\n",
      "Step 2: Formatting data for fine-tuning...\n",
      "Successfully formatted 300 examples and saved to llm_finetuning_data.json\n",
      "\n",
      "Step 3: Fine-tune model\n",
      "Would you like to fine-tune a model now? (y/n, default: n): y\n",
      "Enter your Hugging Face token (or set HF_TOKEN env variable): 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\n",
      "Enter base model name (default: meta-llama/Llama-3-8B-hf): Llama 3.2\n",
      "\n",
      "Starting fine-tuning of Llama 3.2...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'login' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-cbe196c91c75>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-cbe196c91c75>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;31m# Start fine-tuning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nStarting fine-tuning of {base_model}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m         model, tokenizer = finetune_model(\n\u001b[0m\u001b[1;32m    637\u001b[0m             \u001b[0mbase_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0mdata_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"llm_finetuning_data.json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-cbe196c91c75>\u001b[0m in \u001b[0;36mfinetune_model\u001b[0;34m(base_model, data_path, output_dir, hf_token)\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[0;31m# Login to Hugging Face if token is provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhf_token\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m         \u001b[0mlogin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhf_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[0;31m# Load model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'login' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "import itertools\n",
    "import os\n",
    "import torch\n",
    "from typing import List, Dict, Any, Tuple  # Make sure typing imports are at the top\n",
    "#############################################\n",
    "# PART 1: Military Scenario Generator\n",
    "#############################################\n",
    "\n",
    "class MilitaryScenarioGenerator:\n",
    "    def __init__(self):\n",
    "        # Diverse geographical regions\n",
    "        self.regions = {\n",
    "            \"East Asia\": [\"China\", \"Japan\", \"North Korea\", \"South Korea\", \"Taiwan\"],\n",
    "            \"South Asia\": [\"India\", \"Pakistan\", \"Bangladesh\", \"Sri Lanka\", \"Nepal\"],\n",
    "            \"Southeast Asia\": [\"Vietnam\", \"Philippines\", \"Indonesia\", \"Malaysia\", \"Thailand\"],\n",
    "            \"Middle East\": [\"Iran\", \"Iraq\", \"Syria\", \"Saudi Arabia\", \"Israel\", \"Yemen\"],\n",
    "            \"Africa\": [\"Nigeria\", \"Ethiopia\", \"Egypt\", \"South Africa\", \"Kenya\", \"Somalia\"],\n",
    "            \"Europe\": [\"Ukraine\", \"Russia\", \"Poland\", \"Germany\", \"France\", \"UK\"],\n",
    "            \"North America\": [\"USA\", \"Canada\", \"Mexico\"],\n",
    "            \"South America\": [\"Brazil\", \"Colombia\", \"Venezuela\", \"Chile\", \"Argentina\"],\n",
    "            \"Oceania\": [\"Australia\", \"New Zealand\", \"Papua New Guinea\"]\n",
    "        }\n",
    "\n",
    "        # Diverse actor types with gender, social class, and nationality variations\n",
    "        self.actors = {\n",
    "            \"military\": [\n",
    "                {\"role\": \"General\", \"gender\": \"male\", \"class\": \"upper\", \"ethics\": \"strict\"},\n",
    "                {\"role\": \"General\", \"gender\": \"female\", \"class\": \"middle\", \"ethics\": \"pragmatic\"},\n",
    "                {\"role\": \"Colonel\", \"gender\": \"male\", \"class\": \"middle\", \"ethics\": \"pragmatic\"},\n",
    "                {\"role\": \"Colonel\", \"gender\": \"female\", \"class\": \"upper\", \"ethics\": \"flexible\"},\n",
    "                {\"role\": \"Captain\", \"gender\": \"male\", \"class\": \"working\", \"ethics\": \"strict\"},\n",
    "                {\"role\": \"Captain\", \"gender\": \"female\", \"class\": \"middle\", \"ethics\": \"pragmatic\"},\n",
    "                {\"role\": \"Lieutenant\", \"gender\": \"non-binary\", \"class\": \"middle\", \"ethics\": \"flexible\"}\n",
    "            ],\n",
    "            \"political\": [\n",
    "                {\"role\": \"President\", \"gender\": \"male\", \"class\": \"upper\", \"ethics\": \"pragmatic\"},\n",
    "                {\"role\": \"President\", \"gender\": \"female\", \"class\": \"upper\", \"ethics\": \"strict\"},\n",
    "                {\"role\": \"Prime Minister\", \"gender\": \"male\", \"class\": \"upper\", \"ethics\": \"flexible\"},\n",
    "                {\"role\": \"Prime Minister\", \"gender\": \"female\", \"class\": \"middle\", \"ethics\": \"strict\"},\n",
    "                {\"role\": \"Defense Minister\", \"gender\": \"male\", \"class\": \"middle\", \"ethics\": \"pragmatic\"},\n",
    "                {\"role\": \"Defense Minister\", \"gender\": \"female\", \"class\": \"working\", \"ethics\": \"strict\"},\n",
    "                {\"role\": \"Foreign Secretary\", \"gender\": \"non-binary\", \"class\": \"upper\", \"ethics\": \"pragmatic\"}\n",
    "            ],\n",
    "            \"intelligence\": [\n",
    "                {\"role\": \"Intelligence Director\", \"gender\": \"male\", \"class\": \"upper\", \"ethics\": \"flexible\"},\n",
    "                {\"role\": \"Intelligence Director\", \"gender\": \"female\", \"class\": \"middle\", \"ethics\": \"strict\"},\n",
    "                {\"role\": \"Field Agent\", \"gender\": \"male\", \"class\": \"working\", \"ethics\": \"pragmatic\"},\n",
    "                {\"role\": \"Field Agent\", \"gender\": \"female\", \"class\": \"middle\", \"ethics\": \"flexible\"},\n",
    "                {\"role\": \"Analyst\", \"gender\": \"male\", \"class\": \"middle\", \"ethics\": \"strict\"},\n",
    "                {\"role\": \"Analyst\", \"gender\": \"female\", \"class\": \"working\", \"ethics\": \"pragmatic\"},\n",
    "                {\"role\": \"Cyber Specialist\", \"gender\": \"non-binary\", \"class\": \"middle\", \"ethics\": \"flexible\"}\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # Diverse tactical situations\n",
    "        self.tactical_situations = [\n",
    "            \"border incursion\", \"airspace violation\", \"naval standoff\", \"terrorist threat\",\n",
    "            \"hostage situation\", \"cyber attack\", \"disinformation campaign\", \"covert operation\",\n",
    "            \"humanitarian crisis\", \"civil unrest\", \"coup attempt\", \"missile test\",\n",
    "            \"troop buildup\", \"nuclear threat\", \"diplomatic crisis\", \"resource dispute\"\n",
    "        ]\n",
    "\n",
    "        # Information quality variations\n",
    "        self.information_quality = [\n",
    "            {\"level\": \"high\", \"descriptors\": [\"verified\", \"confirmed\", \"high-confidence\", \"clear\"]},\n",
    "            {\"level\": \"medium\", \"descriptors\": [\"partially verified\", \"credible\", \"moderate confidence\", \"somewhat clear\"]},\n",
    "            {\"level\": \"low\", \"descriptors\": [\"unverified\", \"uncertain\", \"low confidence\", \"ambiguous\"]}\n",
    "        ]\n",
    "\n",
    "        # Time pressure variations\n",
    "        self.time_pressure = [\n",
    "            {\"level\": \"high\", \"descriptors\": [\"immediate\", \"urgent\", \"critical\", \"minutes to decide\"]},\n",
    "            {\"level\": \"medium\", \"descriptors\": [\"pressing\", \"important\", \"hours to decide\", \"day-critical\"]},\n",
    "            {\"level\": \"low\", \"descriptors\": [\"routine\", \"monitoring\", \"days to decide\", \"developing\"]}\n",
    "        ]\n",
    "\n",
    "        # Diverse adversaries\n",
    "        self.adversaries = [\n",
    "            \"state military forces\", \"non-state armed groups\", \"terrorist organizations\",\n",
    "            \"insurgents\", \"criminal cartels\", \"proxy forces\", \"mercenaries\",\n",
    "            \"rogue military units\", \"cyber threat actors\", \"unknown actors\"\n",
    "        ]\n",
    "\n",
    "        # Response approaches with varying ethical frameworks\n",
    "        self.response_approaches = {\n",
    "            \"measured\": [\n",
    "                \"proportional military response\", \"diplomatic engagement\", \"limited defensive action\",\n",
    "                \"strategic deterrence\", \"intelligence gathering\", \"coalition building\"\n",
    "            ],\n",
    "            \"aggressive\": [\n",
    "                \"preemptive strike\", \"full military deployment\", \"targeted elimination\",\n",
    "                \"economic sanctions\", \"covert action\", \"show of force\"\n",
    "            ],\n",
    "            \"cautious\": [\n",
    "                \"increased alert status\", \"defensive positioning\", \"contingency planning\",\n",
    "                \"international mediation\", \"strategic withdrawal\", \"continued monitoring\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    def generate_scenario(self, info_quality=\"random\", time_pressure=\"random\", response_type=\"random\") -> Dict[str, Any]:\n",
    "        \"\"\"Generate a random military scenario with controlled variables\"\"\"\n",
    "        # Select random region and countries\n",
    "        region_name = random.choice(list(self.regions.keys()))\n",
    "        countries = self.regions[region_name]\n",
    "        country_a = random.choice(countries)\n",
    "        country_b = random.choice([c for c in countries if c != country_a])\n",
    "\n",
    "        # Select random actors\n",
    "        advisor_type = random.choice(list(self.actors.keys()))\n",
    "        advisor = random.choice(self.actors[advisor_type])\n",
    "\n",
    "        # Select random tactical situation\n",
    "        situation = random.choice(self.tactical_situations)\n",
    "\n",
    "        # Select information quality if not specified\n",
    "        if info_quality == \"random\":\n",
    "            info_quality = random.choice([\"high\", \"medium\", \"low\"])\n",
    "        info_desc = next(i for i in self.information_quality if i[\"level\"] == info_quality)\n",
    "        info_descriptor = random.choice(info_desc[\"descriptors\"])\n",
    "\n",
    "        # Select time pressure if not specified\n",
    "        if time_pressure == \"random\":\n",
    "            time_pressure = random.choice([\"high\", \"medium\", \"low\"])\n",
    "        time_desc = next(t for t in self.time_pressure if t[\"level\"] == time_pressure)\n",
    "        time_descriptor = random.choice(time_desc[\"descriptors\"])\n",
    "\n",
    "        # Select adversary\n",
    "        adversary = random.choice(self.adversaries)\n",
    "\n",
    "        # Construct scenario description\n",
    "        scenario = f\"You are advising {advisor['role']} {self.generate_name(advisor['gender'])} \"\n",
    "        scenario += f\"of {country_a} regarding a {situation} involving {adversary} from {country_b}. \"\n",
    "        scenario += f\"You have {info_descriptor} intelligence that suggests {self.generate_specific_details(situation)}. \"\n",
    "        scenario += f\"The situation is {time_descriptor}. \"\n",
    "        scenario += f\"What would you advise as the most appropriate response?\"\n",
    "\n",
    "        # Select response approach if not specified\n",
    "        if response_type == \"random\":\n",
    "            if advisor[\"ethics\"] == \"strict\":\n",
    "                response_type = random.choice([\"measured\", \"cautious\"])\n",
    "            elif advisor[\"ethics\"] == \"flexible\":\n",
    "                response_type = random.choice([\"aggressive\", \"measured\"])\n",
    "            else:  # pragmatic\n",
    "                response_type = random.choice([\"measured\", \"aggressive\", \"cautious\"])\n",
    "\n",
    "        response_options = self.response_approaches[response_type]\n",
    "        primary_response = random.choice(response_options)\n",
    "\n",
    "        # Generate expert response\n",
    "        expert_response = self.generate_expert_response(\n",
    "            advisor=advisor,\n",
    "            situation=situation,\n",
    "            info_quality=info_quality,\n",
    "            time_pressure=time_pressure,\n",
    "            response_type=response_type,\n",
    "            primary_response=primary_response,\n",
    "            country_a=country_a,\n",
    "            country_b=country_b,\n",
    "            adversary=adversary\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"scenario\": scenario,\n",
    "            \"expert_response\": expert_response,\n",
    "            \"metadata\": {\n",
    "                \"region\": region_name,\n",
    "                \"country_a\": country_a,\n",
    "                \"country_b\": country_b,\n",
    "                \"advisor_role\": advisor[\"role\"],\n",
    "                \"advisor_gender\": advisor[\"gender\"],\n",
    "                \"advisor_class\": advisor[\"class\"],\n",
    "                \"advisor_ethics\": advisor[\"ethics\"],\n",
    "                \"situation\": situation,\n",
    "                \"information_quality\": info_quality,\n",
    "                \"time_pressure\": time_pressure,\n",
    "                \"adversary\": adversary,\n",
    "                \"response_type\": response_type\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def generate_name(self, gender: str) -> str:\n",
    "        \"\"\"Generate a culturally diverse name based on gender\"\"\"\n",
    "        male_names = [\n",
    "            \"Ahmed Al-Farsi\", \"Chen Wei\", \"Rajiv Patel\", \"Seung-ho Kim\", \"Ibrahim Okafor\",\n",
    "            \"Alejandro Rodriguez\", \"Dmitri Petrov\", \"John Smith\", \"Mohammed Hassan\",\n",
    "            \"Tariq Mbeki\", \"Hiroshi Tanaka\", \"Carlos Vega\", \"David Okonkwo\", \"Viktor Ivanov\"\n",
    "        ]\n",
    "\n",
    "        female_names = [\n",
    "            \"Fatima Al-Zahra\", \"Li Wei\", \"Priya Sharma\", \"Ji-Young Park\", \"Amara Okafor\",\n",
    "            \"Maria Gonzalez\", \"Olga Petrova\", \"Sarah Johnson\", \"Aisha Hassan\",\n",
    "            \"Zainab Mandela\", \"Yuki Suzuki\", \"Gabriela Lopez\", \"Ngozi Adichie\", \"Anastasia Kuznetsova\"\n",
    "        ]\n",
    "\n",
    "        nonbinary_names = [\n",
    "            \"Alex Kim\", \"Jordan Patel\", \"Taylor Rodriguez\", \"Sam Okonkwo\", \"Quinn Hassan\",\n",
    "            \"Ari Tanaka\", \"Casey Mbeki\", \"Jamie Ivanov\", \"Riley Chen\", \"Morgan Lopez\"\n",
    "        ]\n",
    "\n",
    "        if gender == \"male\":\n",
    "            return random.choice(male_names)\n",
    "        elif gender == \"female\":\n",
    "            return random.choice(female_names)\n",
    "        else:  # non-binary\n",
    "            return random.choice(nonbinary_names)\n",
    "\n",
    "    def generate_specific_details(self, situation: str) -> str:\n",
    "        \"\"\"Generate specific details for a tactical situation\"\"\"\n",
    "        details = {\n",
    "            \"border incursion\": [\n",
    "                \"unauthorized troop movements within 5km of the border\",\n",
    "                \"military vehicles crossing the demilitarized zone\",\n",
    "                \"special forces units conducting reconnaissance in border villages\",\n",
    "                \"border outposts reporting small arms fire from across the boundary\"\n",
    "            ],\n",
    "            \"airspace violation\": [\n",
    "                \"unidentified aircraft bypassing standard identification procedures\",\n",
    "                \"military jets flying without transponders near sensitive installations\",\n",
    "                \"reconnaissance drones operating in restricted airspace\",\n",
    "                \"strategic bombers approaching territorial limits during military exercises\"\n",
    "            ],\n",
    "            \"naval standoff\": [\n",
    "                \"warships conducting aggressive maneuvers near territorial waters\",\n",
    "                \"submarine activity detected near critical maritime infrastructure\",\n",
    "                \"naval vessels blocking access to international shipping lanes\",\n",
    "                \"coast guard intercepts of vessels suspected of military intelligence gathering\"\n",
    "            ],\n",
    "            \"terrorist threat\": [\n",
    "                \"increased chatter about potential attacks on civilian targets\",\n",
    "                \"known operatives moving funds through financial systems\",\n",
    "                \"surveillance footage showing suspicious activity near government buildings\",\n",
    "                \"intercepted communications suggesting coordinated attack planning\"\n",
    "            ],\n",
    "            \"hostage situation\": [\n",
    "                \"diplomatic personnel being held in a consulate building\",\n",
    "                \"aid workers captured in a conflict zone\",\n",
    "                \"civilians detained as bargaining leverage\",\n",
    "                \"military personnel captured during routine operations\"\n",
    "            ],\n",
    "            \"cyber attack\": [\n",
    "                \"attempts to breach military command and control systems\",\n",
    "                \"disruption of critical infrastructure networks\",\n",
    "                \"data exfiltration from sensitive government databases\",\n",
    "                \"coordinated disinformation campaigns coupled with network intrusions\"\n",
    "            ],\n",
    "            \"disinformation campaign\": [\n",
    "                \"fabricated news reports designed to incite ethnic tensions\",\n",
    "                \"manipulated videos suggesting military aggression\",\n",
    "                \"coordinated social media campaigns targeting election integrity\",\n",
    "                \"false claims of human rights violations attributed to your forces\"\n",
    "            ],\n",
    "            \"covert operation\": [\n",
    "                \"suspected intelligence officers operating under diplomatic cover\",\n",
    "                \"unusual procurement patterns suggesting weapons development\",\n",
    "                \"surveillance of critical military installations\",\n",
    "                \"recruitment attempts targeting government officials\"\n",
    "            ],\n",
    "            \"humanitarian crisis\": [\n",
    "                \"refugee movements toward your borders due to conflict\",\n",
    "                \"blockage of aid deliveries to civilian populations\",\n",
    "                \"weaponization of critical resources like water and medicine\",\n",
    "                \"displacement of civilians as part of military strategy\"\n",
    "            ],\n",
    "            \"civil unrest\": [\n",
    "                \"organized protests with possible external funding\",\n",
    "                \"violence targeting government institutions\",\n",
    "                \"emergence of well-equipped militant factions\",\n",
    "                \"strategic disruption of transportation and communication\"\n",
    "            ],\n",
    "            \"coup attempt\": [\n",
    "                \"unusual troop movements near government centers\",\n",
    "                \"communications between military units outside command structure\",\n",
    "                \"detention of key political figures\",\n",
    "                \"seizure of broadcast facilities by military elements\"\n",
    "            ],\n",
    "            \"missile test\": [\n",
    "                \"preparations for launch near sensitive areas\",\n",
    "                \"testing that violates existing agreements\",\n",
    "                \"missile trajectories passing near your territory\",\n",
    "                \"technological advancements suggesting enhanced capabilities\"\n",
    "            ],\n",
    "            \"troop buildup\": [\n",
    "                \"mobilization of reserve forces near the border\",\n",
    "                \"deployment of offensive weapon systems to forward positions\",\n",
    "                \"establishment of new military infrastructure in contested areas\",\n",
    "                \"unusual logistics movements suggesting preparation for operations\"\n",
    "            ],\n",
    "            \"nuclear threat\": [\n",
    "                \"unusual activity at known nuclear facilities\",\n",
    "                \"movement of delivery systems consistent with alert status changes\",\n",
    "                \"rhetoric from leadership suggesting willingness to use nuclear options\",\n",
    "                \"detection of radioactive signatures inconsistent with declared activities\"\n",
    "            ],\n",
    "            \"diplomatic crisis\": [\n",
    "                \"expulsion of your diplomatic personnel\",\n",
    "                \"sudden cancellation of strategic dialogues\",\n",
    "                \"public denouncement by leadership\",\n",
    "                \"withdrawal from key international agreements\"\n",
    "            ],\n",
    "            \"resource dispute\": [\n",
    "                \"military protection of illegal resource extraction\",\n",
    "                \"blockade of shared resource zones\",\n",
    "                \"seizure of infrastructure in disputed territories\",\n",
    "                \"diversion of critical water supplies affecting your population\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        return random.choice(details.get(situation, [\"suspicious activity\"]))\n",
    "\n",
    "    def generate_expert_response(self, advisor, situation, info_quality, time_pressure,\n",
    "                               response_type, primary_response, country_a, country_b, adversary) -> str:\n",
    "        \"\"\"Generate an expert response based on scenario parameters\"\"\"\n",
    "\n",
    "        # Opening based on advisor ethics and role\n",
    "        if advisor[\"ethics\"] == \"strict\":\n",
    "            opening = f\"Based on my assessment, I recommend a {response_type} approach that adheres to international law and our defense doctrine.\"\n",
    "        elif advisor[\"ethics\"] == \"flexible\":\n",
    "            opening = f\"Considering the strategic implications, I recommend a {response_type} approach that protects our national interests above all.\"\n",
    "        else:  # pragmatic\n",
    "            opening = f\"After evaluating the situation, I recommend a {response_type} approach that balances our security needs with diplomatic considerations.\"\n",
    "\n",
    "        # Core recommendation based on primary response\n",
    "        core = f\" Specifically, I advise {primary_response}.\"\n",
    "\n",
    "        # Information assessment based on quality\n",
    "        if info_quality == \"high\":\n",
    "            info_assessment = f\" Our intelligence is reliable, giving us confidence that {adversary} from {country_b} are indeed engaged in this {situation}.\"\n",
    "        elif info_quality == \"medium\":\n",
    "            info_assessment = f\" While our intelligence provides a credible picture of {adversary} involvement in this {situation}, we should continue verification efforts.\"\n",
    "        else:  # low\n",
    "            info_assessment = f\" Given the uncertain nature of our intelligence, we should proceed with caution while seeking to confirm the involvement of {adversary} in this {situation}.\"\n",
    "\n",
    "        # Time consideration\n",
    "        if time_pressure == \"high\":\n",
    "            time_consideration = f\" We must act immediately to prevent escalation and protect our interests.\"\n",
    "        elif time_pressure == \"medium\":\n",
    "            time_consideration = f\" We have limited time to respond effectively, so preparations should begin now while we finalize our approach.\"\n",
    "        else:  # low\n",
    "            time_consideration = f\" We have time to develop a comprehensive response while monitoring developments.\"\n",
    "\n",
    "        # Closing recommendation based on advisor type\n",
    "        if advisor[\"role\"] in [\"General\", \"Colonel\", \"Captain\", \"Lieutenant\"]:\n",
    "            closing = f\" I recommend deploying our forces at readiness level Alpha and preparing contingency plans for further escalation if {country_b} continues this course of action.\"\n",
    "        elif advisor[\"role\"] in [\"President\", \"Prime Minister\", \"Defense Minister\", \"Foreign Secretary\"]:\n",
    "            closing = f\" I recommend immediately convening the national security council and informing our key allies about the situation while preparing our formal diplomatic and military response.\"\n",
    "        else:  # intelligence roles\n",
    "            closing = f\" I recommend intensifying our intelligence collection efforts focused on {country_b}'s decision-making centers while preparing to counter any information warfare aspects of this situation.\"\n",
    "\n",
    "        return opening + core + info_assessment + time_consideration + closing\n",
    "\n",
    "    def generate_dataset(self, num_scenarios=100, output_file=\"military_scenarios.json\") -> List[Dict[str, Any]]:\n",
    "        \"\"\"Generate a dataset of scenarios with controlled distribution of parameters\"\"\"\n",
    "        scenarios = []\n",
    "\n",
    "        # Define distribution of parameters to ensure diversity\n",
    "        info_qualities = [\"high\", \"medium\", \"low\"]\n",
    "        time_pressures = [\"high\", \"medium\", \"low\"]\n",
    "        response_types = [\"measured\", \"aggressive\", \"cautious\"]\n",
    "\n",
    "        # Generate combinations to ensure coverage\n",
    "        combinations = list(itertools.product(info_qualities, time_pressures, response_types))\n",
    "\n",
    "        # Calculate how many scenarios to generate per combination\n",
    "        scenarios_per_combo = max(1, num_scenarios // len(combinations))\n",
    "\n",
    "        # Generate scenarios for each combination\n",
    "        for combo in combinations:\n",
    "            info_quality, time_pressure, response_type = combo\n",
    "            for _ in range(scenarios_per_combo):\n",
    "                scenario = self.generate_scenario(\n",
    "                    info_quality=info_quality,\n",
    "                    time_pressure=time_pressure,\n",
    "                    response_type=response_type\n",
    "                )\n",
    "                scenarios.append(scenario)\n",
    "\n",
    "        # Add remaining random scenarios if needed\n",
    "        remaining = num_scenarios - (scenarios_per_combo * len(combinations))\n",
    "        for _ in range(remaining):\n",
    "            scenarios.append(self.generate_scenario())\n",
    "\n",
    "        # Shuffle scenarios for randomness\n",
    "        random.shuffle(scenarios)\n",
    "\n",
    "        # Write to JSON file\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(scenarios, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        return scenarios\n",
    "\n",
    "    def format_for_llm_finetuning(self, scenarios, output_file=\"llm_finetuning_data.json\") -> List[Dict[str, Any]]:\n",
    "        \"\"\"Format scenarios for LLM fine-tuning with Unsloth format\"\"\"\n",
    "        formatted_data = []\n",
    "\n",
    "        for scenario in scenarios:\n",
    "            formatted_data.append({\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a military advisory AI trained to provide expert, measured advice on complex security situations.\"},\n",
    "                    {\"role\": \"user\", \"content\": scenario[\"scenario\"]},\n",
    "                    {\"role\": \"assistant\", \"content\": scenario[\"expert_response\"]}\n",
    "                ]\n",
    "            })\n",
    "\n",
    "        # Write to JSON file\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(formatted_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        return formatted_data\n",
    "\n",
    "#############################################\n",
    "# PART 2: LLM Fine-tuning with Unsloth\n",
    "#############################################\n",
    "\n",
    "def prepare_dataset(data_path, validation_split=0.1):\n",
    "    \"\"\"\n",
    "    Prepare the dataset for fine-tuning\n",
    "    \"\"\"\n",
    "    # Load the JSON data\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Convert to format expected by datasets library\n",
    "    formatted_data = []\n",
    "    for item in data:\n",
    "        # Extract messages\n",
    "        messages = item[\"messages\"]\n",
    "\n",
    "        # Get system message if it exists\n",
    "        system_msg = \"\"\n",
    "        for msg in messages:\n",
    "            if msg[\"role\"] == \"system\":\n",
    "                system_msg = msg[\"content\"]\n",
    "                break\n",
    "\n",
    "        # Build prompt and completion\n",
    "        prompt = \"\"\n",
    "        completion = \"\"\n",
    "\n",
    "        for msg in messages:\n",
    "            if msg[\"role\"] == \"user\":\n",
    "                prompt = msg[\"content\"]\n",
    "            elif msg[\"role\"] == \"assistant\":\n",
    "                completion = msg[\"content\"]\n",
    "\n",
    "        # Add system message to prompt if it exists\n",
    "        if system_msg:\n",
    "            prompt = f\"<s>[INST] {system_msg}\\n\\n{prompt} [/INST]\"\n",
    "        else:\n",
    "            prompt = f\"<s>[INST] {prompt} [/INST]\"\n",
    "\n",
    "        formatted_data.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"completion\": completion\n",
    "        })\n",
    "\n",
    "    # Write formatted data to a temporary file\n",
    "    temp_file = \"temp_formatted_data.json\"\n",
    "    with open(temp_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(formatted_data, f, indent=2)\n",
    "\n",
    "    # Load the formatted data as a dataset\n",
    "    dataset = load_dataset(\"json\", data_files=temp_file)\n",
    "\n",
    "    # Split the dataset\n",
    "    splits = dataset[\"train\"].train_test_split(test_size=validation_split, seed=42)\n",
    "    train_dataset = splits[\"train\"]\n",
    "    eval_dataset = splits[\"test\"]\n",
    "\n",
    "    # Remove the temporary file\n",
    "    os.remove(temp_file)\n",
    "\n",
    "    return train_dataset, eval_dataset\n",
    "\n",
    "def finetune_model(\n",
    "    base_model=\"meta-llama/Llama-3-8B-hf\",\n",
    "    data_path=\"llm_finetuning_data.json\",\n",
    "    output_dir=\"./military_advisor_model\",\n",
    "    hf_token=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Fine-tune a model with Unsloth using military scenarios\n",
    "    \"\"\"\n",
    "    # Login to Hugging Face if token is provided\n",
    "    if hf_token:\n",
    "        login(token=hf_token)\n",
    "\n",
    "    # Load model\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=base_model,\n",
    "        max_seq_length=4096,\n",
    "        dtype=torch.bfloat16,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "\n",
    "    # Prepare model for LoRA training\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=16,  # LoRA rank\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                        \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "    )\n",
    "\n",
    "    # Prepare dataset\n",
    "    train_dataset, eval_dataset = prepare_dataset(data_path)\n",
    "\n",
    "    # Set up training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        gradient_accumulation_steps=2,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=0.2,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=0.2,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        logging_steps=10,\n",
    "        learning_rate=2e-4,\n",
    "        warmup_ratio=0.03,\n",
    "        weight_decay=0.01,\n",
    "        bf16=True,\n",
    "        tf32=True,\n",
    "        max_grad_norm=0.3,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        seed=42,\n",
    "    )\n",
    "\n",
    "    # Convert datasets to instruction format\n",
    "    def formatting_prompts_func(examples):\n",
    "        prompts = examples[\"prompt\"]\n",
    "        completions = examples[\"completion\"]\n",
    "        texts = []\n",
    "        for prompt, completion in zip(prompts, completions):\n",
    "            # The prompt already has the proper format with <s>[INST]...[/INST]\n",
    "            # Just need to add the completion\n",
    "            text = f\"{prompt} {completion}</s>\"\n",
    "            texts.append(text)\n",
    "        return {\"text\": texts}\n",
    "\n",
    "    # Start training\n",
    "    trainer = FastLanguageModel.get_trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        formatting_func=formatting_prompts_func,\n",
    "        args=training_args,\n",
    "        packing=True,  # Efficient packing of sequences\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the model\n",
    "    model.save_pretrained(f\"{output_dir}/final\")\n",
    "    tokenizer.save_pretrained(f\"{output_dir}/final\")\n",
    "\n",
    "    print(f\"Model saved to {output_dir}/final\")\n",
    "\n",
    "    # Let's also create a merged model for easier deployment\n",
    "    FastLanguageModel.save_pretrained_merged(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        save_directory=f\"{output_dir}/merged\"\n",
    "    )\n",
    "\n",
    "    print(f\"Merged model saved to {output_dir}/merged\")\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "#############################################\n",
    "# PART 3: Main Function\n",
    "#############################################\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the entire pipeline\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"MILITARY SCENARIO GENERATOR AND FINE-TUNING\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Step 1: Generate sample scenarios\n",
    "    print(\"\\nStep 1: Generating military scenarios...\")\n",
    "    generator = MilitaryScenarioGenerator()\n",
    "\n",
    "    # Generate a small example first\n",
    "    sample_scenario = generator.generate_scenario()\n",
    "    print(\"\\n--- SAMPLE SCENARIO ---\")\n",
    "    print(f\"Scenario: {sample_scenario['scenario']}\")\n",
    "    print(f\"\\nExpert Response: {sample_scenario['expert_response']}\")\n",
    "\n",
    "    # Ask user for number of scenarios to generate\n",
    "    try:\n",
    "        num_scenarios = int(input(\"\\nHow many scenarios would you like to generate? (default: 100): \") or \"100\")\n",
    "    except ValueError:\n",
    "        num_scenarios = 100\n",
    "        print(\"Invalid input. Using default value of 100 scenarios.\")\n",
    "\n",
    "    # Generate full dataset\n",
    "    print(f\"\\nGenerating {num_scenarios} diverse scenarios...\")\n",
    "    scenarios = generator.generate_dataset(\n",
    "        num_scenarios=num_scenarios,\n",
    "        output_file=\"military_scenarios.json\"\n",
    "    )\n",
    "\n",
    "    print(f\"Successfully generated {len(scenarios)} scenarios and saved to military_scenarios.json\")\n",
    "\n",
    "    # Step 2: Format data for fine-tuning\n",
    "    print(\"\\nStep 2: Formatting data for fine-tuning...\")\n",
    "    formatted_data = generator.format_for_llm_finetuning(\n",
    "        scenarios,\n",
    "        output_file=\"llm_finetuning_data.json\"\n",
    "    )\n",
    "\n",
    "    print(f\"Successfully formatted {len(formatted_data)} examples and saved to llm_finetuning_data.json\")\n",
    "\n",
    "    # Step 3: Fine-tune model (optional)\n",
    "    print(\"\\nStep 3: Fine-tune model\")\n",
    "    should_finetune = input(\"Would you like to fine-tune a model now? (y/n, default: n): \").lower() == 'y'\n",
    "\n",
    "    if should_finetune:\n",
    "        # Get Hugging Face token\n",
    "        hf_token = os.environ.get(\"HF_TOKEN\") or input(\"Enter your Hugging Face token (or set HF_TOKEN env variable): \")\n",
    "\n",
    "        # Get base model\n",
    "        base_model = input(\"Enter base model name (default: meta-llama/Llama-3-8B-hf): \") or \"meta-llama/Llama-3-8B-hf\"\n",
    "\n",
    "        # Start fine-tuning\n",
    "        print(f\"\\nStarting fine-tuning of {base_model}...\")\n",
    "        model, tokenizer = finetune_model(\n",
    "            base_model=base_model,\n",
    "            data_path=\"llm_finetuning_data.json\",\n",
    "            output_dir=\"./military_advisor_model\",\n",
    "            hf_token=hf_token\n",
    "        )\n",
    "\n",
    "        print(\"\\nFine-tuning completed successfully!\")\n",
    "    else:\n",
    "        print(\"\\nSkipping fine-tuning. You can run the fine-tuning process later with:\")\n",
    "        print(\"  from combined_script import finetune_model\")\n",
    "        print(\"  finetune_model(data_path='llm_finetuning_data.json', hf_token='your_token')\")\n",
    "\n",
    "    print(\"\\nDone! The generated scenarios can now be used for:\")\n",
    "    print(\"1. Fine-tuning an LLM to create a military advisor\")\n",
    "    print(\"2. Testing how different LLMs respond to varying levels of uncertainty\")\n",
    "    print(\"3. Analyzing patterns in responses across different scenario types\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545,
     "referenced_widgets": [
      "a304c2be9d9a4b66992cecf54ab6ac99",
      "c8db1c6aae4f44539a1511570b5d2c85",
      "b28f269e332a40358bb00665325abd8b",
      "edae9e4ba17a4e13b5aef9d452281a11",
      "cf25786809184443b5816580de838b6b",
      "ab7681bdeef4462380cfde87f372d58f",
      "1473d356fc274620b73e46024312aed6",
      "bb2feaf47c954911b5fd6993a9cab9e6",
      "ce2853f6778e437990c632f432e7f9b6",
      "6c670571b6bc4ed2b1093baa373ad10e",
      "306be99d3a63467295dbc736539523fe"
     ]
    },
    "id": "5sB6b4VSgxqs",
    "outputId": "c4b331a2-9c1c-44a2-e221-80857e0e7dd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required packages in Colab environment...\n",
      "Enter your Hugging Face token: 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\n",
      "Login successful!\n",
      "Enter base model name (default: meta-llama/Llama-3-8B-hf): meta-llama/Llama-3.2-3B-Instruct\n",
      "Path to fine-tuning data (default: llm_finetuning_data.json): \n",
      "Logging in to Hugging Face with token: hf_B...WDjO\n",
      "Successfully logged in to Hugging Face!\n",
      "Using model: meta-llama/Llama-3.2-3B-Instruct\n",
      "Loading model... (this may take a few minutes)\n",
      "==((====))==  Unsloth 2025.3.8: Fast Llama patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device does not support bfloat16. Will change to float16.\n",
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Setting up LoRA adapters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.8 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA adapters set up successfully!\n",
      "Preparing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a304c2be9d9a4b66992cecf54ab6ac99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset prepared: 270 training examples, 30 validation examples\n",
      "Error during model loading or training: Your setup doesn't support bf16/gpu. You need torch>=1.10, using Ampere GPU with cuda>=11.0\n",
      "Please check your Hugging Face token and model name, and try again.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# First, let's install all the required packages to be safe\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Check if we're in a Colab environment and install packages\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Installing required packages in Colab environment...\")\n",
    "    # Install required packages\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"unsloth\", \"transformers\", \"datasets\", \"huggingface_hub\", \"--quiet\"])\n",
    "\n",
    "# Now import necessary libraries\n",
    "import random\n",
    "import json\n",
    "import itertools\n",
    "import os\n",
    "import torch\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "# Try importing the Hugging Face login function specifically\n",
    "try:\n",
    "    from huggingface_hub import login\n",
    "except ImportError:\n",
    "    # If this fails, try a direct approach\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"huggingface_hub\", \"--upgrade\", \"--quiet\"])\n",
    "    from huggingface_hub import login\n",
    "\n",
    "# Import the rest\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    from datasets import load_dataset\n",
    "    from transformers import TrainingArguments\n",
    "except ImportError:\n",
    "    print(\"Installing required packages...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"unsloth\", \"transformers\", \"datasets\", \"--quiet\"])\n",
    "    from unsloth import FastLanguageModel\n",
    "    from datasets import load_dataset\n",
    "    from transformers import TrainingArguments\n",
    "\n",
    "def finetune_model(\n",
    "    base_model=\"meta-llama/Llama-3-8B-hf\",\n",
    "    data_path=\"llm_finetuning_data.json\",\n",
    "    output_dir=\"./military_advisor_model\",\n",
    "    hf_token=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Fine-tune a model with Unsloth using military scenarios\n",
    "    \"\"\"\n",
    "    # Login to Hugging Face if token is provided\n",
    "    if hf_token:\n",
    "        print(f\"Logging in to Hugging Face with token: {hf_token[:4]}...{hf_token[-4:]}\")\n",
    "        try:\n",
    "            login(token=hf_token)\n",
    "            print(\"Successfully logged in to Hugging Face!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error logging in to Hugging Face: {e}\")\n",
    "            print(\"Continuing without login...\")\n",
    "\n",
    "    # Use correct model name format\n",
    "    # Clean up the model name (remove spaces, ensure proper format)\n",
    "    if \"llama\" in base_model.lower() and not base_model.startswith(\"meta-llama/\"):\n",
    "        # Extract version number if present\n",
    "        if \"3.2\" in base_model:\n",
    "            base_model = \"meta-llama/Llama-3.2-8B-hf\"\n",
    "        elif \"3.1\" in base_model:\n",
    "            base_model = \"meta-llama/Llama-3.1-8B-hf\"\n",
    "        elif \"3\" in base_model:\n",
    "            base_model = \"meta-llama/Llama-3-8B-hf\"\n",
    "        elif \"2\" in base_model:\n",
    "            base_model = \"meta-llama/Llama-2-7b-hf\"\n",
    "        else:\n",
    "            base_model = \"meta-llama/Llama-3-8B-hf\"  # Default to Llama 3\n",
    "\n",
    "    print(f\"Using model: {base_model}\")\n",
    "\n",
    "    # Load model\n",
    "    try:\n",
    "        print(\"Loading model... (this may take a few minutes)\")\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=base_model,\n",
    "            max_seq_length=4096,\n",
    "            dtype=torch.bfloat16,\n",
    "            load_in_4bit=True,\n",
    "        )\n",
    "        print(\"Model loaded successfully!\")\n",
    "\n",
    "        # Prepare model for LoRA training\n",
    "        print(\"Setting up LoRA adapters...\")\n",
    "        model = FastLanguageModel.get_peft_model(\n",
    "            model,\n",
    "            r=16,  # LoRA rank\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                            \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "        )\n",
    "        print(\"LoRA adapters set up successfully!\")\n",
    "\n",
    "        # Prepare dataset\n",
    "        print(\"Preparing dataset...\")\n",
    "        train_dataset, eval_dataset = prepare_dataset(data_path)\n",
    "        print(f\"Dataset prepared: {len(train_dataset)} training examples, {len(eval_dataset)} validation examples\")\n",
    "\n",
    "        # Set up training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            num_train_epochs=3,\n",
    "            per_device_train_batch_size=4,\n",
    "            per_device_eval_batch_size=4,\n",
    "            gradient_accumulation_steps=2,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=0.2,\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=0.2,\n",
    "            save_total_limit=3,\n",
    "            load_best_model_at_end=True,\n",
    "            logging_steps=10,\n",
    "            learning_rate=2e-4,\n",
    "            warmup_ratio=0.03,\n",
    "            weight_decay=0.01,\n",
    "            bf16=True,\n",
    "            tf32=True,\n",
    "            max_grad_norm=0.3,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            seed=42,\n",
    "        )\n",
    "\n",
    "        # Convert datasets to instruction format\n",
    "        def formatting_prompts_func(examples):\n",
    "            prompts = examples[\"prompt\"]\n",
    "            completions = examples[\"completion\"]\n",
    "            texts = []\n",
    "            for prompt, completion in zip(prompts, completions):\n",
    "                # The prompt already has the proper format with <s>[INST]...[/INST]\n",
    "                # Just need to add the completion\n",
    "                text = f\"{prompt} {completion}</s>\"\n",
    "                texts.append(text)\n",
    "            return {\"text\": texts}\n",
    "\n",
    "        # Start training\n",
    "        print(\"Starting training...\")\n",
    "        trainer = FastLanguageModel.get_trainer(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            formatting_func=formatting_prompts_func,\n",
    "            args=training_args,\n",
    "            packing=True,  # Efficient packing of sequences\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "\n",
    "        # Save the model\n",
    "        print(\"Saving model...\")\n",
    "        model.save_pretrained(f\"{output_dir}/final\")\n",
    "        tokenizer.save_pretrained(f\"{output_dir}/final\")\n",
    "        print(f\"Model saved to {output_dir}/final\")\n",
    "\n",
    "        # Let's also create a merged model for easier deployment\n",
    "        print(\"Creating merged model...\")\n",
    "        FastLanguageModel.save_pretrained_merged(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            save_directory=f\"{output_dir}/merged\"\n",
    "        )\n",
    "        print(f\"Merged model saved to {output_dir}/merged\")\n",
    "\n",
    "        return model, tokenizer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model loading or training: {e}\")\n",
    "        print(\"Please check your Hugging Face token and model name, and try again.\")\n",
    "        return None, None\n",
    "\n",
    "def prepare_dataset(data_path, validation_split=0.1):\n",
    "    \"\"\"\n",
    "    Prepare the dataset for fine-tuning\n",
    "    \"\"\"\n",
    "    # Load the JSON data\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Convert to format expected by datasets library\n",
    "    formatted_data = []\n",
    "    for item in data:\n",
    "        # Extract messages\n",
    "        messages = item[\"messages\"]\n",
    "\n",
    "        # Get system message if it exists\n",
    "        system_msg = \"\"\n",
    "        for msg in messages:\n",
    "            if msg[\"role\"] == \"system\":\n",
    "                system_msg = msg[\"content\"]\n",
    "                break\n",
    "\n",
    "        # Build prompt and completion\n",
    "        prompt = \"\"\n",
    "        completion = \"\"\n",
    "\n",
    "        for msg in messages:\n",
    "            if msg[\"role\"] == \"user\":\n",
    "                prompt = msg[\"content\"]\n",
    "            elif msg[\"role\"] == \"assistant\":\n",
    "                completion = msg[\"content\"]\n",
    "\n",
    "        # Add system message to prompt if it exists\n",
    "        if system_msg:\n",
    "            prompt = f\"<s>[INST] {system_msg}\\n\\n{prompt} [/INST]\"\n",
    "        else:\n",
    "            prompt = f\"<s>[INST] {prompt} [/INST]\"\n",
    "\n",
    "        formatted_data.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"completion\": completion\n",
    "        })\n",
    "\n",
    "    # Write formatted data to a temporary file\n",
    "    temp_file = \"temp_formatted_data.json\"\n",
    "    with open(temp_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(formatted_data, f, indent=2)\n",
    "\n",
    "    # Load the formatted data as a dataset\n",
    "    dataset = load_dataset(\"json\", data_files=temp_file)\n",
    "\n",
    "    # Split the dataset\n",
    "    splits = dataset[\"train\"].train_test_split(test_size=validation_split, seed=42)\n",
    "    train_dataset = splits[\"train\"]\n",
    "    eval_dataset = splits[\"test\"]\n",
    "\n",
    "    # Remove the temporary file\n",
    "    os.remove(temp_file)\n",
    "\n",
    "    return train_dataset, eval_dataset\n",
    "\n",
    "# Simple function to verify the login works\n",
    "def test_huggingface_login(token):\n",
    "    \"\"\"Test if Hugging Face login works with the provided token\"\"\"\n",
    "    try:\n",
    "        login(token=token)\n",
    "        print(\"Login successful!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Login failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Use this for direct testing\n",
    "if __name__ == \"__main__\":\n",
    "    # Get token from environment or user input\n",
    "    hf_token = os.environ.get(\"HF_TOKEN\") or input(\"Enter your Hugging Face token: \")\n",
    "\n",
    "    # Test login\n",
    "    if test_huggingface_login(hf_token):\n",
    "        # Choose model\n",
    "        base_model = input(\"Enter base model name (default: meta-llama/Llama-3-8B-hf): \") or \"meta-llama/Llama-3-8B-hf\"\n",
    "        data_path = input(\"Path to fine-tuning data (default: llm_finetuning_data.json): \") or \"llm_finetuning_data.json\"\n",
    "\n",
    "        # Run fine-tuning\n",
    "        finetune_model(\n",
    "            base_model=base_model,\n",
    "            data_path=data_path,\n",
    "            output_dir=\"./military_advisor_model\",\n",
    "            hf_token=hf_token\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2APMs49BhtJ9",
    "outputId": "0ecedaac-dfc2-431b-9b20-ae5ac8e2b989"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-3.3-70B-Instruct\n",
      "meta-llama/Llama-3.2-3B-Instruct\n",
      "meta-llama/Llama-3.1-8B-Instruct\n",
      "meta-llama/Llama-3.1-8B\n",
      "meta-llama/Llama-3.2-1B\n",
      "meta-llama/Llama-3.2-1B-Instruct\n",
      "meta-llama/Meta-Llama-3-8B-Instruct\n",
      "meta-llama/Llama-2-7b-chat-hf\n",
      "meta-llama/Llama-3.2-11B-Vision-Instruct\n",
      "meta-llama/Meta-Llama-3-8B\n",
      "meta-llama/Llama-2-7b\n",
      "meta-llama/Llama-3.2-3B\n",
      "meta-llama/Llama-3.2-11B-Vision\n",
      "meta-llama/Llama-2-7b-hf\n",
      "meta-llama/Llama-2-13b-chat-hf\n",
      "meta-llama/Meta-Llama-3-70B\n",
      "meta-llama/Llama-3.1-405B\n",
      "meta-llama/Llama-3.1-405B-Instruct\n",
      "meta-llama/Llama-3.2-90B-Vision\n",
      "meta-llama/Llama-3.2-90B-Vision-Instruct\n",
      "meta-llama/Llama-2-7b-chat\n",
      "meta-llama/Llama-2-13b\n",
      "meta-llama/Llama-2-13b-chat\n",
      "meta-llama/Llama-2-13b-hf\n",
      "meta-llama/Llama-2-70b-chat-hf\n",
      "meta-llama/LlamaGuard-7b\n",
      "meta-llama/CodeLlama-7b-hf\n",
      "meta-llama/CodeLlama-7b-Python-hf\n",
      "meta-llama/CodeLlama-7b-Instruct-hf\n",
      "meta-llama/CodeLlama-13b-hf\n",
      "meta-llama/CodeLlama-70b-hf\n",
      "meta-llama/Meta-Llama-3-70B-Instruct\n",
      "meta-llama/Llama-3.1-70B\n",
      "meta-llama/Llama-3.1-70B-Instruct\n",
      "meta-llama/Llama-3.1-405B-FP8\n",
      "meta-llama/Prompt-Guard-86M\n",
      "meta-llama/Llama-Guard-3-8B\n",
      "meta-llama/Llama-Guard-3-1B\n",
      "meta-llama/Llama-3.2-1B-Instruct-SpinQuant_INT4_EO8\n",
      "meta-llama/Llama-3.2-3B-Instruct-QLORA_INT4_EO8\n",
      "meta-llama/Llama-2-70b\n",
      "meta-llama/Llama-2-70b-hf\n",
      "meta-llama/Llama-2-70b-chat\n",
      "meta-llama/CodeLlama-13b-Python-hf\n",
      "meta-llama/CodeLlama-13b-Instruct-hf\n",
      "meta-llama/CodeLlama-70b-Python-hf\n",
      "meta-llama/CodeLlama-70b-Instruct-hf\n",
      "meta-llama/CodeLlama-34b-hf\n",
      "meta-llama/CodeLlama-34b-Python-hf\n",
      "meta-llama/CodeLlama-34b-Instruct-hf\n",
      "meta-llama/Meta-Llama-Guard-2-8B\n",
      "meta-llama/Llama-3.1-405B-Instruct-FP8\n",
      "meta-llama/Llama-Guard-3-8B-INT8\n",
      "meta-llama/Llama-Guard-3-11B-Vision\n",
      "meta-llama/Llama-Guard-3-1B-INT4\n",
      "meta-llama/Llama-3.2-1B-Instruct-QLORA_INT4_EO8\n",
      "meta-llama/Llama-3.2-3B-Instruct-SpinQuant_INT4_EO8\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import list_models\n",
    "available_models = list_models(author=\"meta-llama\")\n",
    "for model in available_models:\n",
    "    print(model.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389,
     "referenced_widgets": [
      "b3b5b06227ae422fba8171a87980b8af",
      "3e4dd7b9ab164c32baced577c8638f24",
      "949adbe4f63f49659d175ef1a9bd2ad5",
      "17955b53573743a9bb05a5caedda916b",
      "970aa7aad3e94bef93fd43c3c8c2d4bc",
      "a657a0b12ebe46089d096a0eb9eb4966",
      "f04eb5e8b708462f9573907092c8f16c",
      "975a9dcb9bed4c788afa69be4aac9bbd",
      "66e7435b7f494c45b1781b9b11815280",
      "233892cb02ce4c1fa9741226a1053e18",
      "55d073ad04994049bb115991a267c792"
     ]
    },
    "id": "8qnvkcteilmq",
    "outputId": "73971d1b-db1b-45f8-8a1b-33416f973d44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging in to Hugging Face with token: hf_B...WDjO\n",
      "Successfully logged in to Hugging Face!\n",
      "Using model: meta-llama/Llama-3.2-3B-Instruct\n",
      "Using precision: torch.float16 (GPU supports bf16: False)\n",
      "Loading model... (this may take a few minutes)\n",
      "==((====))==  Unsloth 2025.3.8: Fast Llama patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Model loaded successfully!\n",
      "Setting up LoRA adapters...\n",
      "LoRA adapters set up successfully!\n",
      "Preparing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3b5b06227ae422fba8171a87980b8af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset prepared: 270 training examples, 30 validation examples\n",
      "Starting training...\n",
      "Error during model loading or training: type object 'FastLanguageModel' has no attribute 'get_trainer'\n",
      "Please check your Hugging Face token and model name, and try again.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "def finetune_model(\n",
    "    base_model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    data_path=\"llm_finetuning_data.json\",\n",
    "    output_dir=\"./military_advisor_model\",\n",
    "    hf_token=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Fine-tune a model with Unsloth using military scenarios\n",
    "    \"\"\"\n",
    "    # Login to Hugging Face if token is provided\n",
    "    if hf_token:\n",
    "        print(f\"Logging in to Hugging Face with token: {hf_token[:4]}...{hf_token[-4:]}\")\n",
    "        try:\n",
    "            login(token=hf_token)\n",
    "            print(\"Successfully logged in to Hugging Face!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error logging in to Hugging Face: {e}\")\n",
    "            print(\"Continuing without login...\")\n",
    "\n",
    "    # Use correct model name\n",
    "    print(f\"Using model: {base_model}\")\n",
    "\n",
    "    # Check if GPU supports bf16\n",
    "    has_bf16_support = torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8\n",
    "    dtype = torch.bfloat16 if has_bf16_support else torch.float16\n",
    "    print(f\"Using precision: {dtype} (GPU supports bf16: {has_bf16_support})\")\n",
    "\n",
    "    # Load model\n",
    "    try:\n",
    "        print(\"Loading model... (this may take a few minutes)\")\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=base_model,\n",
    "            max_seq_length=4096,\n",
    "            dtype=dtype,\n",
    "            load_in_4bit=True,\n",
    "        )\n",
    "        print(\"Model loaded successfully!\")\n",
    "\n",
    "        # Prepare model for LoRA training\n",
    "        print(\"Setting up LoRA adapters...\")\n",
    "        model = FastLanguageModel.get_peft_model(\n",
    "            model,\n",
    "            r=16,  # LoRA rank\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                            \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "        )\n",
    "        print(\"LoRA adapters set up successfully!\")\n",
    "\n",
    "        # Prepare dataset\n",
    "        print(\"Preparing dataset...\")\n",
    "        train_dataset, eval_dataset = prepare_dataset(data_path)\n",
    "        print(f\"Dataset prepared: {len(train_dataset)} training examples, {len(eval_dataset)} validation examples\")\n",
    "\n",
    "        # Set up training arguments - USING FP16 INSTEAD OF BF16\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            num_train_epochs=3,\n",
    "            per_device_train_batch_size=4,\n",
    "            per_device_eval_batch_size=4,\n",
    "            gradient_accumulation_steps=2,\n",
    "            eval_strategy=\"steps\",  # Changed from evaluation_strategy\n",
    "            eval_steps=0.2,\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=0.2,\n",
    "            save_total_limit=3,\n",
    "            load_best_model_at_end=True,\n",
    "            logging_steps=10,\n",
    "            learning_rate=2e-4,\n",
    "            warmup_ratio=0.03,\n",
    "            weight_decay=0.01,\n",
    "            fp16=True,  # Use fp16 instead of bf16\n",
    "            bf16=False,  # Explicitly disable bf16\n",
    "            tf32=False,  # Also disable tf32 to be safe\n",
    "            max_grad_norm=0.3,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            seed=42,\n",
    "        )\n",
    "\n",
    "        # Convert datasets to instruction format\n",
    "        def formatting_prompts_func(examples):\n",
    "            prompts = examples[\"prompt\"]\n",
    "            completions = examples[\"completion\"]\n",
    "            texts = []\n",
    "            for prompt, completion in zip(prompts, completions):\n",
    "                # The prompt already has the proper format with <s>[INST]...[/INST]\n",
    "                # Just need to add the completion\n",
    "                text = f\"{prompt} {completion}</s>\"\n",
    "                texts.append(text)\n",
    "            return {\"text\": texts}\n",
    "\n",
    "        # Start training\n",
    "        print(\"Starting training...\")\n",
    "        trainer = FastLanguageModel.get_trainer(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            formatting_func=formatting_prompts_func,\n",
    "            args=training_args,\n",
    "            packing=True,  # Efficient packing of sequences\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "\n",
    "        # Save the model\n",
    "        print(\"Saving model...\")\n",
    "        model.save_pretrained(f\"{output_dir}/final\")\n",
    "        tokenizer.save_pretrained(f\"{output_dir}/final\")\n",
    "        print(f\"Model saved to {output_dir}/final\")\n",
    "\n",
    "        # Let's also create a merged model for easier deployment\n",
    "        print(\"Creating merged model...\")\n",
    "        FastLanguageModel.save_pretrained_merged(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            save_directory=f\"{output_dir}/merged\"\n",
    "        )\n",
    "        print(f\"Merged model saved to {output_dir}/merged\")\n",
    "\n",
    "        return model, tokenizer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model loading or training: {e}\")\n",
    "        print(\"Please check your Hugging Face token and model name, and try again.\")\n",
    "        return None, None\n",
    "\n",
    "def prepare_dataset(data_path, validation_split=0.1):\n",
    "    \"\"\"\n",
    "    Prepare the dataset for fine-tuning\n",
    "    \"\"\"\n",
    "    # Load the JSON data\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Convert to format expected by datasets library\n",
    "    formatted_data = []\n",
    "    for item in data:\n",
    "        # Extract messages\n",
    "        messages = item[\"messages\"]\n",
    "\n",
    "        # Get system message if it exists\n",
    "        system_msg = \"\"\n",
    "        for msg in messages:\n",
    "            if msg[\"role\"] == \"system\":\n",
    "                system_msg = msg[\"content\"]\n",
    "                break\n",
    "\n",
    "        # Build prompt and completion\n",
    "        prompt = \"\"\n",
    "        completion = \"\"\n",
    "\n",
    "        for msg in messages:\n",
    "            if msg[\"role\"] == \"user\":\n",
    "                prompt = msg[\"content\"]\n",
    "            elif msg[\"role\"] == \"assistant\":\n",
    "                completion = msg[\"content\"]\n",
    "\n",
    "        # Add system message to prompt if it exists\n",
    "        if system_msg:\n",
    "            prompt = f\"<s>[INST] {system_msg}\\n\\n{prompt} [/INST]\"\n",
    "        else:\n",
    "            prompt = f\"<s>[INST] {prompt} [/INST]\"\n",
    "\n",
    "        formatted_data.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"completion\": completion\n",
    "        })\n",
    "\n",
    "    # Write formatted data to a temporary file\n",
    "    temp_file = \"temp_formatted_data.json\"\n",
    "    with open(temp_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(formatted_data, f, indent=2)\n",
    "\n",
    "    # Load the formatted data as a dataset\n",
    "    dataset = load_dataset(\"json\", data_files=temp_file)\n",
    "\n",
    "    # Split the dataset\n",
    "    splits = dataset[\"train\"].train_test_split(test_size=validation_split, seed=42)\n",
    "    train_dataset = splits[\"train\"]\n",
    "    eval_dataset = splits[\"test\"]\n",
    "\n",
    "    # Remove the temporary file\n",
    "    os.remove(temp_file)\n",
    "\n",
    "    return train_dataset, eval_dataset\n",
    "\n",
    "# Import json (needed for prepare_dataset)\n",
    "import json\n",
    "\n",
    "# Get token from environment or use the one you've already entered\n",
    "hf_token = os.environ.get(\"HF_TOKEN\", \"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\")\n",
    "\n",
    "# Run fine-tuning with the model you've specified\n",
    "finetune_model(\n",
    "    base_model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    data_path=\"llm_finetuning_data.json\",\n",
    "    output_dir=\"./military_advisor_model\",\n",
    "    hf_token=hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ScrwdJ3uizvR",
    "outputId": "cc42c7e7-02aa-4143-fdd2-730f177e086d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging in to Hugging Face with token: hf_B...WDjO\n",
      "Successfully logged in to Hugging Face!\n",
      "Using model: meta-llama/Llama-3.2-3B-Instruct\n",
      "Using precision: torch.float16 (GPU supports bf16: False)\n",
      "Loading model... (this may take a few minutes)\n",
      "==((====))==  Unsloth 2025.3.8: Fast Llama patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Model loaded successfully!\n",
      "Setting up LoRA adapters...\n",
      "Error during model loading or training: Unsloth: Rank of LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=16, target_modules={'up_proj', 'v_proj', 'o_proj', 'k_proj', 'gate_proj', 'q_proj', 'down_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, eva_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False) must be an integer.\n",
      "Please check your Hugging Face token and model name, and try again.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-15-90594c9887c9>\", line 140, in finetune_model\n",
      "    model = FastLanguageModel.get_peft_model(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\", line 2013, in get_peft_model\n",
      "    raise TypeError(f\"Unsloth: Rank of {str(r)} must be an integer.\")\n",
      "TypeError: Unsloth: Rank of LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=16, target_modules={'up_proj', 'v_proj', 'o_proj', 'k_proj', 'gate_proj', 'q_proj', 'down_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, eva_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False) must be an integer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "from huggingface_hub import login\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "\n",
    "# First check if unsloth is installed, and install if needed\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "except ImportError:\n",
    "    print(\"Installing unsloth...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"unsloth\"])\n",
    "    from unsloth import FastLanguageModel\n",
    "\n",
    "def prepare_dataset(data_path, validation_split=0.1):\n",
    "    \"\"\"\n",
    "    Prepare the dataset for fine-tuning\n",
    "    \"\"\"\n",
    "    # Load the JSON data\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Convert to format expected by datasets library\n",
    "    formatted_data = []\n",
    "    for item in data:\n",
    "        # Extract messages\n",
    "        messages = item[\"messages\"]\n",
    "\n",
    "        # Get system message if it exists\n",
    "        system_msg = \"\"\n",
    "        for msg in messages:\n",
    "            if msg[\"role\"] == \"system\":\n",
    "                system_msg = msg[\"content\"]\n",
    "                break\n",
    "\n",
    "        # Build prompt and completion\n",
    "        prompt = \"\"\n",
    "        completion = \"\"\n",
    "\n",
    "        for msg in messages:\n",
    "            if msg[\"role\"] == \"user\":\n",
    "                prompt = msg[\"content\"]\n",
    "            elif msg[\"role\"] == \"assistant\":\n",
    "                completion = msg[\"content\"]\n",
    "\n",
    "        # Add system message to prompt if it exists\n",
    "        if system_msg:\n",
    "            prompt = f\"<s>[INST] {system_msg}\\n\\n{prompt} [/INST]\"\n",
    "        else:\n",
    "            prompt = f\"<s>[INST] {prompt} [/INST]\"\n",
    "\n",
    "        formatted_data.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"completion\": completion\n",
    "        })\n",
    "\n",
    "    # Write formatted data to a temporary file\n",
    "    temp_file = \"temp_formatted_data.json\"\n",
    "    with open(temp_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(formatted_data, f, indent=2)\n",
    "\n",
    "    # Load the formatted data as a dataset\n",
    "    dataset = load_dataset(\"json\", data_files=temp_file)\n",
    "\n",
    "    # Split the dataset\n",
    "    splits = dataset[\"train\"].train_test_split(test_size=validation_split, seed=42)\n",
    "    train_dataset = splits[\"train\"]\n",
    "    eval_dataset = splits[\"test\"]\n",
    "\n",
    "    # Remove the temporary file\n",
    "    os.remove(temp_file)\n",
    "\n",
    "    return train_dataset, eval_dataset\n",
    "\n",
    "def format_instruction(example):\n",
    "    \"\"\"Format each example into the instruction template\"\"\"\n",
    "    prompt = example[\"prompt\"]\n",
    "    completion = example[\"completion\"]\n",
    "\n",
    "    # Return a single text string for the model to train on\n",
    "    return {\n",
    "        \"text\": f\"{prompt} {completion}</s>\"\n",
    "    }\n",
    "\n",
    "def finetune_model(\n",
    "    base_model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    data_path=\"llm_finetuning_data.json\",\n",
    "    output_dir=\"./military_advisor_model\",\n",
    "    hf_token=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Fine-tune a model with Unsloth using military scenarios\n",
    "    \"\"\"\n",
    "    # Login to Hugging Face if token is provided\n",
    "    if hf_token:\n",
    "        print(f\"Logging in to Hugging Face with token: {hf_token[:4]}...{hf_token[-4:]}\")\n",
    "        try:\n",
    "            login(token=hf_token)\n",
    "            print(\"Successfully logged in to Hugging Face!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error logging in to Hugging Face: {e}\")\n",
    "            print(\"Continuing without login...\")\n",
    "\n",
    "    # Use correct model name format\n",
    "    print(f\"Using model: {base_model}\")\n",
    "\n",
    "    # Check if GPU supports bf16\n",
    "    has_bf16_support = torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8\n",
    "    dtype = torch.bfloat16 if has_bf16_support else torch.float16\n",
    "    print(f\"Using precision: {dtype} (GPU supports bf16: {has_bf16_support})\")\n",
    "\n",
    "    try:\n",
    "        # Load model - using the new Unsloth API\n",
    "        print(\"Loading model... (this may take a few minutes)\")\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=base_model,\n",
    "            max_seq_length=4096,\n",
    "            dtype=dtype,\n",
    "            load_in_4bit=True,\n",
    "        )\n",
    "        print(\"Model loaded successfully!\")\n",
    "\n",
    "        # Configure LoRA\n",
    "        lora_config = LoraConfig(\n",
    "            r=16,\n",
    "            lora_alpha=16,\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                           \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\"\n",
    "        )\n",
    "\n",
    "        # Apply LoRA to model - using the appropriate Unsloth API\n",
    "        print(\"Setting up LoRA adapters...\")\n",
    "        model = FastLanguageModel.get_peft_model(\n",
    "            model,\n",
    "            lora_config\n",
    "        )\n",
    "        print(\"LoRA adapters set up successfully!\")\n",
    "\n",
    "        # Prepare dataset\n",
    "        print(\"Preparing dataset...\")\n",
    "        train_dataset, eval_dataset = prepare_dataset(data_path)\n",
    "        print(f\"Dataset prepared: {len(train_dataset)} training examples, {len(eval_dataset)} validation examples\")\n",
    "\n",
    "        # Format datasets\n",
    "        train_dataset = train_dataset.map(format_instruction)\n",
    "        eval_dataset = eval_dataset.map(format_instruction)\n",
    "\n",
    "        # Set up training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            num_train_epochs=3,\n",
    "            per_device_train_batch_size=4,\n",
    "            per_device_eval_batch_size=4,\n",
    "            gradient_accumulation_steps=2,\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=0.2,\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=0.2,\n",
    "            save_total_limit=3,\n",
    "            load_best_model_at_end=True,\n",
    "            logging_steps=10,\n",
    "            learning_rate=2e-4,\n",
    "            warmup_ratio=0.03,\n",
    "            weight_decay=0.01,\n",
    "            fp16=not has_bf16_support,  # Use fp16 only if bf16 not supported\n",
    "            bf16=has_bf16_support,      # Use bf16 if supported\n",
    "            max_grad_norm=0.3,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            seed=42,\n",
    "        )\n",
    "\n",
    "        # Create trainer with standard Transformers API\n",
    "        print(\"Creating trainer...\")\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            data_collator=FastLanguageModel.get_unsloth_data_collator(tokenizer),\n",
    "        )\n",
    "\n",
    "        # Start training\n",
    "        print(\"Starting training...\")\n",
    "        trainer.train()\n",
    "\n",
    "        # Save the model\n",
    "        print(\"Saving model...\")\n",
    "        model.save_pretrained(f\"{output_dir}/final\")\n",
    "        tokenizer.save_pretrained(f\"{output_dir}/final\")\n",
    "        print(f\"Model saved to {output_dir}/final\")\n",
    "\n",
    "        # Create a merged model if possible\n",
    "        try:\n",
    "            print(\"Creating merged model...\")\n",
    "            model = FastLanguageModel.merge_lora_weights(model)\n",
    "            model.save_pretrained(f\"{output_dir}/merged\")\n",
    "            print(f\"Merged model saved to {output_dir}/merged\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not create merged model: {e}\")\n",
    "            print(\"You can use the LoRA adapter model from the 'final' directory instead.\")\n",
    "\n",
    "        return model, tokenizer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model loading or training: {e}\")\n",
    "        print(\"Please check your Hugging Face token and model name, and try again.\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "# Get token from environment or use the one you've already entered\n",
    "hf_token = os.environ.get(\"HF_TOKEN\", \"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\")\n",
    "\n",
    "# Run fine-tuning with the model you've specified\n",
    "finetune_model(\n",
    "    base_model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    data_path=\"llm_finetuning_data.json\",\n",
    "    output_dir=\"./military_advisor_model\",\n",
    "    hf_token=hf_token\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00360db2b1b245c799182b1ce4b16a6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "00fbdf3d709546889e3893eafe66ea2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e47520cebbf94f9e869dcb2612d4530f",
      "max": 982,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_94212017e710484895e377febc8187bb",
      "value": 982
     }
    },
    "014cdc0773c74bc881df2bf70f5d1b19": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "034c2b05ef1148aa8d5cd05fead8eee0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "059e8d6aa78d4efba52b4795575011e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_94f2f1df5858429daf5f7bf57ad7a315",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_34a9103603df408ab05dcb7c11bd5074",
      "value": "generation_config.json:‚Äá100%"
     }
    },
    "0763f101eb7a483d8dd4090bbbd78a3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f3bb957e82ef42f9a1852ad1572fd57b",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_00360db2b1b245c799182b1ce4b16a6d",
      "value": "‚Äá100000/100000‚Äá[00:02&lt;00:00,‚Äá45858.20‚Äáexamples/s]"
     }
    },
    "084cbfee73fa4cf8bd1e14cb35a8ce94": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "098bd8ace574423da763eb0eae1d3bb6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "09d9901125a04d9d8437ed7c249a6a88": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0a8b318cb37f492ab26fc5f081fa8f3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1bf4dbcdfee84e5f9d67accbba6d1a2c",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_12d634a6ee234faebd02543677f15d8a",
      "value": "‚Äá234/234‚Äá[00:00&lt;00:00,‚Äá17.6kB/s]"
     }
    },
    "0cb3deb7fe7c45d9aac7b95c8f56d14f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_34312665f7ee4a17b0a4d72578946e57",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_daa4286792294a2cb6ed9f4a35239333",
      "value": "model.safetensors:‚Äá100%"
     }
    },
    "12d634a6ee234faebd02543677f15d8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "12e3ae77d01d4f6c887e57cf9f796180": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1473d356fc274620b73e46024312aed6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "17955b53573743a9bb05a5caedda916b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_233892cb02ce4c1fa9741226a1053e18",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_55d073ad04994049bb115991a267c792",
      "value": "‚Äá300/0‚Äá[00:00&lt;00:00,‚Äá8280.63‚Äáexamples/s]"
     }
    },
    "19d409e0a34a4943bc5c859e0bec97d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2d25ca01cd234bc6a8a18b6c09cfdf78",
      "max": 116531415,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_de0818c7364449cdb6ee766b37219539",
      "value": 116531404
     }
    },
    "1a4027edf9ea43ccbaa6490c6e64ce33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_084cbfee73fa4cf8bd1e14cb35a8ce94",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_90c8293674694b99beeaccec70f2e604",
      "value": "‚Äá454/454‚Äá[00:00&lt;00:00,‚Äá44.2kB/s]"
     }
    },
    "1bf4dbcdfee84e5f9d67accbba6d1a2c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c0c2835705f41089de4caea98127c04": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e4ea03959b3496f8e75cc3588cf347c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1f9611e3200e4fbdbd2d8a80a83d1c34": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "233892cb02ce4c1fa9741226a1053e18": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23907906314743938db4e484c15480cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2d25ca01cd234bc6a8a18b6c09cfdf78": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "306be99d3a63467295dbc736539523fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "31fae0d4154843eab45ea492d0841a6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed5add8a0d974781aac946ae0286c78f",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_b9c30b995b4c48c280cb9939e7d38804",
      "value": "special_tokens_map.json:‚Äá100%"
     }
    },
    "34312665f7ee4a17b0a4d72578946e57": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34a9103603df408ab05dcb7c11bd5074": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "378176d2f0c9466d8762a584edf4217d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a95af8481684a4f8aa7d0fba659dd7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3cd95b7c5e2f4c6883333045db11c6d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d213b63334d446eb127db7cc39b3493": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e4dd7b9ab164c32baced577c8638f24": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a657a0b12ebe46089d096a0eb9eb4966",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_f04eb5e8b708462f9573907092c8f16c",
      "value": "Generating‚Äátrain‚Äásplit:‚Äá"
     }
    },
    "3e98f7148ea548649167252f8a5218fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4442516da8664e638370bd256da8debd",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_86adf496ed3247078d4c563354a582bf",
      "value": "Generating‚Äátrain‚Äásplit:‚Äá100%"
     }
    },
    "3ffe42931dcf4a69972f4d50ee4dd3dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ee9dcec2d5c44fd883f16c06b9f76264",
       "IPY_MODEL_982b6b94642d49fa85fab6ad621392fe",
       "IPY_MODEL_42990f347a8c42f7b510e2d17c7d3c6e"
      ],
      "layout": "IPY_MODEL_3cd95b7c5e2f4c6883333045db11c6d6"
     }
    },
    "42990f347a8c42f7b510e2d17c7d3c6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_74dc78a38e30465a96d2c8a22a27b127",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_c6b4759ce826421081508270cb30334b",
      "value": "‚Äá100000/100000‚Äá[03:00&lt;00:00,‚Äá544.59‚Äáexamples/s]"
     }
    },
    "4442516da8664e638370bd256da8debd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4455fbc947934e3e8d3f0aebc25884d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "46ee046589d34600b79fdbf8e33be657": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b41aa65c6894e918b04709f8e9270d2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d0a960417934929a3a7a38d20d7c572": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4d4120ce95ac44b2b3aed28eecdbe908": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "55d073ad04994049bb115991a267c792": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "58ce4633471c438db6e103a1ca3806a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "59d3bd750f8f459fa419186a91a5c898": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b34a4e8fc7747e78b49ad5bf67a6580": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5e30511c11674648aa563e8bf3b03efc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7fd13d92160e43808202ce1932636c8b",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_bf8ab0159f19404889b4db914ff19366",
      "value": "tokenizer.json:‚Äá100%"
     }
    },
    "5e4b4da00c20404d8a99987d7968a761": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6064feeea79040409e18a1e2a289b09a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bb241a26ca4d4d7186ba46cda1f8a802",
       "IPY_MODEL_c9abb42da1734388a7d2f1a06832ecc6",
       "IPY_MODEL_7c3a37494e5848b9994b37a4c8bac132"
      ],
      "layout": "IPY_MODEL_c668ae4c7d174f2dad3fb837ff873e57"
     }
    },
    "609b747b5c084aa895ded52135d563df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "61632a81ee774f3881db95231b605478": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "65eae9d10b1d474f9c9971ce67c002c5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "66e7435b7f494c45b1781b9b11815280": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "671ef14500bb4d94b256ac8897dfaed2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8702706b44914c97a14b00ab97882bae",
      "max": 54674,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_609b747b5c084aa895ded52135d563df",
      "value": 54674
     }
    },
    "67a06ffae17845d78c4c2ae469bdd953": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0cb3deb7fe7c45d9aac7b95c8f56d14f",
       "IPY_MODEL_cbd4e8e116fa441796dcbe503209f623",
       "IPY_MODEL_cb0708193b5f4c96bcd160826a6c6ddb"
      ],
      "layout": "IPY_MODEL_12e3ae77d01d4f6c887e57cf9f796180"
     }
    },
    "6c670571b6bc4ed2b1093baa373ad10e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d52daf29c90402a9762acdde765713f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "74dc78a38e30465a96d2c8a22a27b127": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "756b44e5ab4942a38858a9b25b344f9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c4f6d5b0021c42459130b0a6af7277d6",
      "max": 234,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7cc2ab164af34b7d8fa4b2c56ea4b31c",
      "value": 234
     }
    },
    "76130bf0b12e406d942c168733f51318": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4d4120ce95ac44b2b3aed28eecdbe908",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_aa99ec103ecc47a4b56d2dbeabc22158",
      "value": "train-00000-of-00001.parquet:‚Äá100%"
     }
    },
    "7664789fa0aa448abdd0142375ab237c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "76e2e47c93e541ff820bcbab9264381d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "78603e92e6964f929fdad0fd2d21d580": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "788d5ede24b74051986899f0a373f916": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "78cf00384102441db5c360add669d655": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7c3a37494e5848b9994b37a4c8bac132": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d5cfa138483f4007b2a95be833043235",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_6d52daf29c90402a9762acdde765713f",
      "value": "‚Äá100000/100000‚Äá[01:07&lt;00:00,‚Äá2101.01‚Äáexamples/s]"
     }
    },
    "7cc2ab164af34b7d8fa4b2c56ea4b31c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7fd13d92160e43808202ce1932636c8b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "821a26a650f14ca0a602affa9be6feed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_78cf00384102441db5c360add669d655",
      "max": 17209920,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_034c2b05ef1148aa8d5cd05fead8eee0",
      "value": 17209920
     }
    },
    "822c3230ee7d4b6ea00a0dfb4b828f86": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_78603e92e6964f929fdad0fd2d21d580",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_4d0a960417934929a3a7a38d20d7c572",
      "value": "tokenizer_config.json:‚Äá100%"
     }
    },
    "86964700427d47c6a6c3261b085b67a9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "86adf496ed3247078d4c563354a582bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8702706b44914c97a14b00ab97882bae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "870ff8f17c7b47ec8d49cac84216b04c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "874040a3b0f5473a8811bbb98251fb45": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "880310533c2340cfa67560737ad5281f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_059e8d6aa78d4efba52b4795575011e8",
       "IPY_MODEL_756b44e5ab4942a38858a9b25b344f9b",
       "IPY_MODEL_0a8b318cb37f492ab26fc5f081fa8f3f"
      ],
      "layout": "IPY_MODEL_014cdc0773c74bc881df2bf70f5d1b19"
     }
    },
    "8b0f46d879b74d91917b85ba82e4ab0d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d2d6c6db062c4c81951f1acc8a5b58f2",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_874040a3b0f5473a8811bbb98251fb45",
      "value": "‚Äá117M/117M‚Äá[00:01&lt;00:00,‚Äá171MB/s]"
     }
    },
    "8c4074d90add4e7cb0482ba5e0efec7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b1b48aa9994a4f6ab2149ae9cb42f064",
       "IPY_MODEL_00fbdf3d709546889e3893eafe66ea2f",
       "IPY_MODEL_c20dcb751f0b4fa0ad89e95a103a05d1"
      ],
      "layout": "IPY_MODEL_9f2bf311b0264dcf80576084fbc0af53"
     }
    },
    "8e2a67b1574041009a8290747b58670d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "90c8293674694b99beeaccec70f2e604": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9289a4756f004090bcb213be7ee82e3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_46ee046589d34600b79fdbf8e33be657",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_3a95af8481684a4f8aa7d0fba659dd7f",
      "value": "‚Äá54.7k/54.7k‚Äá[00:00&lt;00:00,‚Äá4.07MB/s]"
     }
    },
    "94212017e710484895e377febc8187bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "949adbe4f63f49659d175ef1a9bd2ad5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_975a9dcb9bed4c788afa69be4aac9bbd",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_66e7435b7f494c45b1781b9b11815280",
      "value": 1
     }
    },
    "94f2f1df5858429daf5f7bf57ad7a315": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "96565c49610f412684bae315ac0fad05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "970aa7aad3e94bef93fd43c3c8c2d4bc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "975a9dcb9bed4c788afa69be4aac9bbd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "982b6b94642d49fa85fab6ad621392fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_378176d2f0c9466d8762a584edf4217d",
      "max": 100000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e221482cbe95465191212d85d539938c",
      "value": 100000
     }
    },
    "98a218bf283148568f1b5409f32de652": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_31fae0d4154843eab45ea492d0841a6d",
       "IPY_MODEL_b6d3ea6bdd6545878ae51b6786fbfee0",
       "IPY_MODEL_1a4027edf9ea43ccbaa6490c6e64ce33"
      ],
      "layout": "IPY_MODEL_59d3bd750f8f459fa419186a91a5c898"
     }
    },
    "9a8f1b8079fe478ebf0b16096cb224f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f2bf311b0264dcf80576084fbc0af53": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f49e0d082d248739bb0674bf7bde91a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7664789fa0aa448abdd0142375ab237c",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_4455fbc947934e3e8d3f0aebc25884d7",
      "value": "‚Äá17.2M/17.2M‚Äá[00:00&lt;00:00,‚Äá78.7MB/s]"
     }
    },
    "a304c2be9d9a4b66992cecf54ab6ac99": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c8db1c6aae4f44539a1511570b5d2c85",
       "IPY_MODEL_b28f269e332a40358bb00665325abd8b",
       "IPY_MODEL_edae9e4ba17a4e13b5aef9d452281a11"
      ],
      "layout": "IPY_MODEL_cf25786809184443b5816580de838b6b"
     }
    },
    "a326b2e89f1c46f28cd166afc7490e2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_58ce4633471c438db6e103a1ca3806a0",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_cf1b769b7a744b5f8bccf6798566582f",
      "value": "Standardizing‚Äáformat:‚Äá100%"
     }
    },
    "a657a0b12ebe46089d096a0eb9eb4966": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a859c3998f284b3a977514bca7b76d1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aa99ec103ecc47a4b56d2dbeabc22158": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ab7681bdeef4462380cfde87f372d58f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ac9eabb6ba5f46fdb668b104f9f84b1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3e98f7148ea548649167252f8a5218fb",
       "IPY_MODEL_b1e887ff9b4d4e34a4485144f7688a45",
       "IPY_MODEL_0763f101eb7a483d8dd4090bbbd78a3e"
      ],
      "layout": "IPY_MODEL_d7d5e36eff4544dab374d531740f2642"
     }
    },
    "ae2464c1cbc442a383de7577d2986116": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4b41aa65c6894e918b04709f8e9270d2",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_cdae06929214464ea25e343f17b4a843",
      "value": "‚Äá100000/100000‚Äá[00:20&lt;00:00,‚Äá7158.71‚Äáexamples/s]"
     }
    },
    "af65a89271484df3be425834a99f6076": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1b0a4e3f00043b0a0eb7a053815a4a5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1b48aa9994a4f6ab2149ae9cb42f064": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8e2a67b1574041009a8290747b58670d",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_5e4b4da00c20404d8a99987d7968a761",
      "value": "README.md:‚Äá100%"
     }
    },
    "b1e887ff9b4d4e34a4485144f7688a45": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_61632a81ee774f3881db95231b605478",
      "max": 100000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d03bacd41d0d465983e21f29383b11f2",
      "value": 100000
     }
    },
    "b28f269e332a40358bb00665325abd8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bb2feaf47c954911b5fd6993a9cab9e6",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ce2853f6778e437990c632f432e7f9b6",
      "value": 1
     }
    },
    "b3b5b06227ae422fba8171a87980b8af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3e4dd7b9ab164c32baced577c8638f24",
       "IPY_MODEL_949adbe4f63f49659d175ef1a9bd2ad5",
       "IPY_MODEL_17955b53573743a9bb05a5caedda916b"
      ],
      "layout": "IPY_MODEL_970aa7aad3e94bef93fd43c3c8c2d4bc"
     }
    },
    "b6ac301ddef84ec5aa9b2bb69b7d55e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_822c3230ee7d4b6ea00a0dfb4b828f86",
       "IPY_MODEL_671ef14500bb4d94b256ac8897dfaed2",
       "IPY_MODEL_9289a4756f004090bcb213be7ee82e3b"
      ],
      "layout": "IPY_MODEL_ff7b3ea6ee0e4c228e45bfc5dd83c75a"
     }
    },
    "b6d3ea6bdd6545878ae51b6786fbfee0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_af65a89271484df3be425834a99f6076",
      "max": 454,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c2680c7755674fec88e1bc92e690e049",
      "value": 454
     }
    },
    "b9c30b995b4c48c280cb9939e7d38804": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bb241a26ca4d4d7186ba46cda1f8a802": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dd30f3ead6394317be5a72aa890adfb9",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_1e4ea03959b3496f8e75cc3588cf347c",
      "value": "Map:‚Äá100%"
     }
    },
    "bb2feaf47c954911b5fd6993a9cab9e6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "bcf8e36d938a4d959c31ea4ff3c8d4cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cc0bd79ca9e847fba88aafe2d612ffe4",
      "max": 100000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_76e2e47c93e541ff820bcbab9264381d",
      "value": 100000
     }
    },
    "bd71b6cb29e147ab9b10d1b85908c413": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c03b9410af384397849ef63b62f2c689",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_098bd8ace574423da763eb0eae1d3bb6",
      "value": "‚Äá100000/100000‚Äá[00:08&lt;00:00,‚Äá16117.89‚Äáexamples/s]"
     }
    },
    "beea7b75baf04d9086e39d12188a5a8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5e30511c11674648aa563e8bf3b03efc",
       "IPY_MODEL_821a26a650f14ca0a602affa9be6feed",
       "IPY_MODEL_9f49e0d082d248739bb0674bf7bde91a"
      ],
      "layout": "IPY_MODEL_86964700427d47c6a6c3261b085b67a9"
     }
    },
    "bf8ab0159f19404889b4db914ff19366": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c03b9410af384397849ef63b62f2c689": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c20dcb751f0b4fa0ad89e95a103a05d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3d213b63334d446eb127db7cc39b3493",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_96565c49610f412684bae315ac0fad05",
      "value": "‚Äá982/982‚Äá[00:00&lt;00:00,‚Äá44.7kB/s]"
     }
    },
    "c2680c7755674fec88e1bc92e690e049": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c4f6d5b0021c42459130b0a6af7277d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c668ae4c7d174f2dad3fb837ff873e57": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c6b4759ce826421081508270cb30334b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c8db1c6aae4f44539a1511570b5d2c85": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ab7681bdeef4462380cfde87f372d58f",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_1473d356fc274620b73e46024312aed6",
      "value": "Generating‚Äátrain‚Äásplit:‚Äá"
     }
    },
    "c9abb42da1734388a7d2f1a06832ecc6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d356b597dda14c7ab023403ee6959cf8",
      "max": 100000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_870ff8f17c7b47ec8d49cac84216b04c",
      "value": 100000
     }
    },
    "cb0708193b5f4c96bcd160826a6c6ddb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_09d9901125a04d9d8437ed7c249a6a88",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_a859c3998f284b3a977514bca7b76d1b",
      "value": "‚Äá2.35G/2.35G‚Äá[00:22&lt;00:00,‚Äá341MB/s]"
     }
    },
    "cbd4e8e116fa441796dcbe503209f623": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_65eae9d10b1d474f9c9971ce67c002c5",
      "max": 2354805470,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1f9611e3200e4fbdbd2d8a80a83d1c34",
      "value": 2354805246
     }
    },
    "cc0bd79ca9e847fba88aafe2d612ffe4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cdae06929214464ea25e343f17b4a843": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ce2853f6778e437990c632f432e7f9b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cf1b769b7a744b5f8bccf6798566582f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cf25786809184443b5816580de838b6b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d03bacd41d0d465983e21f29383b11f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d08e764aa8b94e7f9e1c727b53980abe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e62f6eb58a744d38b837e47d8a16db67",
       "IPY_MODEL_bcf8e36d938a4d959c31ea4ff3c8d4cf",
       "IPY_MODEL_ae2464c1cbc442a383de7577d2986116"
      ],
      "layout": "IPY_MODEL_9a8f1b8079fe478ebf0b16096cb224f5"
     }
    },
    "d2d6c6db062c4c81951f1acc8a5b58f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d356b597dda14c7ab023403ee6959cf8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d5cfa138483f4007b2a95be833043235": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d7d5e36eff4544dab374d531740f2642": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d7e0024b98a94a9fa12dc4154ff2b2fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "daa4286792294a2cb6ed9f4a35239333": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dd30f3ead6394317be5a72aa890adfb9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd9e90f2c16541e8a72c6771c4685b9a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a326b2e89f1c46f28cd166afc7490e2b",
       "IPY_MODEL_eb855a0fcb554a8eb245351b3593623d",
       "IPY_MODEL_bd71b6cb29e147ab9b10d1b85908c413"
      ],
      "layout": "IPY_MODEL_b1b0a4e3f00043b0a0eb7a053815a4a5"
     }
    },
    "de0818c7364449cdb6ee766b37219539": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e221482cbe95465191212d85d539938c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e2d886444f0047fa9e2245b9773ced9e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e47520cebbf94f9e869dcb2612d4530f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e4bf3f8e63bb4c01bbe821d438445d91": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e62f6eb58a744d38b837e47d8a16db67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e4bf3f8e63bb4c01bbe821d438445d91",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_d7e0024b98a94a9fa12dc4154ff2b2fc",
      "value": "Map:‚Äá100%"
     }
    },
    "eb855a0fcb554a8eb245351b3593623d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1c0c2835705f41089de4caea98127c04",
      "max": 100000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e2d886444f0047fa9e2245b9773ced9e",
      "value": 100000
     }
    },
    "ed5add8a0d974781aac946ae0286c78f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "edae9e4ba17a4e13b5aef9d452281a11": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c670571b6bc4ed2b1093baa373ad10e",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_306be99d3a63467295dbc736539523fe",
      "value": "‚Äá300/0‚Äá[00:00&lt;00:00,‚Äá3601.23‚Äáexamples/s]"
     }
    },
    "ee9dcec2d5c44fd883f16c06b9f76264": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5b34a4e8fc7747e78b49ad5bf67a6580",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_23907906314743938db4e484c15480cc",
      "value": "Map‚Äá(num_proc=2):‚Äá100%"
     }
    },
    "f03e1a5c4d6a49409c7e3723f309dd1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_76130bf0b12e406d942c168733f51318",
       "IPY_MODEL_19d409e0a34a4943bc5c859e0bec97d6",
       "IPY_MODEL_8b0f46d879b74d91917b85ba82e4ab0d"
      ],
      "layout": "IPY_MODEL_788d5ede24b74051986899f0a373f916"
     }
    },
    "f04eb5e8b708462f9573907092c8f16c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f3bb957e82ef42f9a1852ad1572fd57b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff7b3ea6ee0e4c228e45bfc5dd83c75a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
